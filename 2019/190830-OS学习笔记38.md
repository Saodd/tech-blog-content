```yaml lw-blog-meta
title: OS学习笔记38：持久化：冗余磁盘阵列RAID
date: "2019-08-30"
brev: 用便宜的磁盘组成阵列~
tags: [OS]
```


# 第三八章 <冗余磁盘阵列 Redundant Arrays of Inexpensive Disks>

[PDF链接](http://pages.cs.wisc.edu/~remzi/OSTEP/file-raid.pdf)

我们对硬盘有三个追求：高速的读写性能、巨大的容量以及扎实的可靠性。

**关键问题：如何制造巨大、快速、可靠的硬盘？有哪些关键技术，有哪些权衡？**

RAID其实就是多块硬盘的结合体。

从外部看来就是一块单块的硬盘。而在内部，它是一个复杂怪兽，有多块硬盘，有内存与固态内存，有一个或多个处理器。一个RAID其实就像一个电脑，专门用来管理硬盘的电脑。

并行访问多块硬盘可以提高性能。多块硬盘组合也提升了容量。同时，将数据分散在多块硬盘上也提升了可靠性；借助一定的冗余，当硬盘故障时也能轻易恢复。

对系统透明的好处就是，易于部署。

## 38.1 接口与内部

对于上层的文件系统来说，RAID就是一个硬盘。当文件系统请求一个逻辑IO时，RAID在内部计算出该地址在哪个硬盘上，然后发起物理IO到相应的硬盘上。

但是，假设RAID中对每个区块有两个副本，当向这样一个`镜像mirrored`RAID中写入时，它将需要同时向两个副本中写入。

RAID一般设计为一个独立的硬件盒子，通过标准连接（比如SCSI或者SATA）连接到宿主机。在内部，包含一些微处理器，内存等元件。

## 38.2 错误模型 Fault Model

RAID在设计时就考虑了侦测并恢复一些种类的硬盘故障。

我们假设的第一个模型非常简单，即`错误即停止fail-stop`模型。在这种情况下，硬盘有两种状态：正常工作与发生故障。对于故障的状态的硬盘，我们假设其中的数据已经完全丢失了。

上述模型还包含了一个假设，即硬盘故障很容易被探测到。因此我们现在不去操心一些『静默故障』或是某个区块的故障；这些故障很复杂，我们稍后再说。

## 38.3 评价指标

- 容量：给定N块硬盘，每个包含B个区块，最后RAID有多少区块可用？
- 可靠性：给定的设计模型能够容忍多少硬盘故障？
- 性能：我们先描述一系列典型的工作负荷。

接下来我们看看三种主要的RAID实现：

## 38.4 RAID0：带状分区

在这种模型下，没有任何冗余，因此甚至不能称为是RAID。它性能最快并且容量最大。最简单的形式就是把区块分成一条一条并映射到每个硬盘上：

![Figure 38.1](../../../tech-blog-pic/2019/2019-08-30-Fig-38-1.png)

我们把每一行称为一`条stripe`。我们可以将条的宽度设为1个区块（4KB），也可以是多个区块：

![Figure 38.2](../../../tech-blog-pic/2019/2019-08-30-Fig-38-2.png)

此时，每个`大块chunk`的大小是8KB，因此一条就是32KB。

### 块尺寸

小的尺寸会让更多的文件有机会散布在多个硬盘上，从而允许更多的并发操作；但是更多并发可能导致更大的延迟，因为延迟是以最大的延迟为准的。

因此块尺寸的选择是建立在权衡的基础上的。一般可能设为64KB，在这里我们为了简化就假设是4KB。

### 评价性能

RAID0性能高，但是有个问题时害怕硬盘故障。

我们假设两种工作负荷，第一种是连续请求10MB，第二种是随机请求一批10KB。硬盘寻道时间7ms，旋转时间3ms，传输速率50MB/s。

那么可得在单硬盘情况下，连续请求速度`S=47.62MB/s`，随机请求速度`R=0.981MB/s`。

而在RAID0情况下，假设有N块硬盘组成阵列，那么连续请求速度`N*S`，随机请求速度也是`N*R`，因为两种情况下我们都可以完全利用所有的硬盘性能。

## 38.5 RAID1：镜像 Mirroring

我们将每个区块复制一份，两个副本分别放在不同的硬盘上。

![Figure 38.3](../../../tech-blog-pic/2019/2019-08-30-Fig-38-3.png)

RAID1又分两种，上图所示这种也称`RAID-10`（或者`RAID 1+0`）是先镜像后分条；还有另一种叫`RAID-01`（或者`RAID 0+1`）是先分条再镜像。

读取的时候可以从任意副本中读取，但是写入的时候要同时写入两个副本。

### 评价

首先容量就减半了。不过容灾率可以达到50%，即能容忍一半的硬盘区块故障。

然后在连续写入时，性能也减半了，只有`N*S/2`，因为我们并发数减半了。在连续读取时，理论上我们依然能够获得巅峰速度`N*S`，因为它内部可以进行分工，所有的硬盘依然可以并行读取；不过有些不能分工的情况，也只有1/2.

随机读写与连续读写情况相同。

## 38.6 RAID4 使用奇偶校验 Parity

使用`奇偶校验Parity`可以节省很多的容量的同时提供一定的可靠性保证，但是代价是性能。

![Figure 38.4](../../../tech-blog-pic/2019/2019-08-30-Fig-38-4.png)

奇偶校验的思想很简单，比如前四块硬盘上储存着0,0,1,1，那么第五块硬盘的对应位置上就储存0；在这种情况下，如果四块硬盘之一发生了区块故障，那么我们可以通过另外三块以及这第五块硬盘做减法来进行恢复。

### 评价

容量还不错，可以提供`(N-1)*B`的容量，损失比较小。可靠性是能够容忍1块硬盘的故障，在5块硬盘的情况下就是20%。

顺序读取可以利用除校验盘以外的所有盘，性能是`(N-1)*S`。在顺序写入的情况下，RAID4会做一个简单的优化，称为`全带宽写入full-stripe write`，即计算出校验位以后，同时写入数据盘与校验盘，因此性能也可以达到`(N-1)*S`。

随机读取也是`(N-1)*R`，但是随机写入有点麻烦。一种方式是`加法校验additive parity`，读取要写入区块相应的另外几块数据盘，然后重新计算校验位，然后再全部写入所有硬盘中。

加法校验太可怕了，我们需要优化，引入`减法校验subtractive parity`。

举个例子，我们有五块硬盘，原来的数据是`0,0,1,1,P=0`；现在我们要写入第三块硬盘，这时候我们读取第三块盘原来的数据`1`以及校验盘的数据`P=0`，如果新写入的数据与原来的数据相等，那么校验位就不变，如果不相等，校验位就改变（二进制，非零即一）。
我们可以用亦或运算符来表达`P[new] = (C[new] ^ C[old]) ^ P[old]`，在整个区块的所有位上运算结束后，同时写入目标数据盘和校验盘。

那么如何计算随机写入场景下的性能？此时，虽然数据盘可以并行读写，但是校验盘只有一个不能并行，因此它会成为瓶颈。由于写入操作既要读又要写，因此性能只有`R/2`。

## 38.7 RAID5：轮流校验 Rotating Parity

思路很简单：既然校验盘会成为瓶颈，那我们把校验区块分散在所有盘上就好了啊！

![Figure 38.7](../../../tech-blog-pic/2019/2019-08-30-Fig-38-7.png)

RAID-5的性能与RAID-4基本相同，最明显的区别就是在于RAID-4的瓶颈：随机写入场景。由于我们将校验区块分散了，因此不同的请求之间将有机会并行，此时总带宽表现为`N*R/4`.

## 38.8 各方案的比较

![Figure 38.8](../../../tech-blog-pic/2019/2019-08-30-Fig-38-8.png)

简单来说，如果你想要最大容量而不关心可靠性，那选择RAID-0；如果你想要极致的随机读写性能，那就选择RAID-1；如果要兼顾容量和性能，那就选择RAID-5。

## 38.9 其他的RAID方案

其实还有RAID-2，RAID-3，RAID-6可以选择，有不同的特性。

当RAID中的硬盘出现故障时，会有一个`热后备hot spare`来顶替故障的部分。

当经历故障时，性能会怎样、如何重建故障的部分？我们需要考虑`潜在区块错误latent sector errors`或者`区块腐化block corruption`。

最后，除了用独立硬件实现RAID，还可以选择软件实现RAID。这种办法很便宜，但是会带来一些额外的问题，比如`一致性写入consistent-update`问题。

## 38.10 小结

总之，RAID就是把多块硬盘组合成为一块，并且对外是透明的，即看起来就是一块硬盘而已。

有很多方案可以选择，不同的方案有不同的取舍，根据情况选择吧。
