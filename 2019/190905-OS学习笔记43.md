```yaml lw-blog-meta
title: OS学习笔记43：持久化：日志结构的文件系统
date: "2019-09-05"
brev: LFS，就是一种copy-on-write的思想，即任何更新都在新的空间中写入。显著地优化随机写入的性能。
tags: [OS]
```


# 第四三章 <Log-structured File Systems>

[PDF链接](http://pages.cs.wisc.edu/~remzi/OSTEP/file-lfs.pdf)

在1990年代开发了新的文件系统`Log-structured File Systems`，开发的动机如下：

- 电脑内存正在增大。内存越大，缓存可以越多。
- 随机读写与顺序读写的性能差异巨大。硬盘的带宽增长迅速，但是物理的电机进步缓慢，因此寻道和旋转时间损耗越来越明显。
- 现有的文件系统对典型负载表现不佳。例如，为了创建一个文件，FFS要做多次读写操作。
- 现有的文件系统对RAID支持不佳。

一个理想的文件系统应该聚焦于写入性能，并且尽可能达到最大的带宽。还能以极低的代价更新节点信息，还要很好地支持RAID。

新的文件系统`LFS`，先将所有的写入数据保存在硬盘设备中的内存块中，当内存满了就申请一个连续写入，将所有数据写到硬盘上。注意，它不会更新已有的数据，它只将内存中的数据写到空闲区域中。因此性能很高。（注意，这里是针对写入的优化，因此会让读取变得不连续。不过参考前面的需求，读取一般都是利用缓存，而且读取也不是性能瓶颈）

**关键问题：如何把所有的写入请求变为顺序写入？**

## 43.1 顺序写入

比如要写入一个数据块，那就直接写！写完了之后再写节点数据，节点指向前面的数据区块。（注意，下图中数据区块一般是4KB，节点区块一般是128bytes）

![Figure 43.1.1](../../../tech-blog-pic/2019/2019-09-05-Fig-43-1-1.png)

## 43.2 高效的顺序写入

如果按照上面的方法，我们先写了一个数据区块，然后稍等了一瞬间再写下一个，那就糟糕了，因为硬盘已经转过去了，我们要等一圈才能接着写。

因此就用`缓冲区buffer`，储存了一些写入请求之后一次性连续写入。这样的一次性写入的一批请求我们称为`段segment`。

![Figure 43.1.2](../../../tech-blog-pic/2019/2019-09-05-Fig-43-1-2.png)

## 43.3 缓冲区应该多大

这个问题应该基于硬盘本身的特性来回答，即磁头定位时间与传输速率的大小关系。

译者注：书上的公式有些麻烦，我觉得用近似的方法就可以体会一下了。例如我们硬盘定位时间10ms，理论极限速度100MB/s，那么需要多少缓存才能达到90%的性能？很简单，大约每次写入时间是90ms就可以了（总时间==10+90==100ms，写入时间占总时间的90%），因此缓存就是100MB/s*90ms==9MB。

## 43.4 问题：寻找节点

如上面的示意图所示，其实也可以想到，最大的麻烦在于如何寻找这些节点？这些节点都任意的写在了数据区块的后面，我们如何找到它们？

回忆一下前面介绍过的文件系统，都是将节点保存在一个固定的区域内，因此只要一个节点编号你就可以找到它在硬盘上的具体位置。

## 43.5 方案：节点地图 The Inode Map

使用多级的`The Inode Map`这种数据结构，实现节点编号->节点地址的映射关系。

那么问题就变成了，如何将这个节点地图持久化？保存在哪里？

一种思路是保存在固定的区域。但是很显然，这会严重影响写入性能，因为它需要频繁更新。
所以解决办法是写在数据区块的后面：

![Figure 43.1.3](../../../tech-blog-pic/2019/2019-09-05-Fig-43-1-3.png)

## 43.6 完善方案：检查点区域

问题又来了，那我们又如何找到节点地图呢？这回必须要有一个固定的区域了。

LFS把这块固定的区域称为`checkpoint region (CR)`，它里面保存着最新的节点地图的地址。为了保证性能，这个检查点区域只会周期性地更新（比如30秒一次），因此对性能影响较小。

![Figure 43.1.4](../../../tech-blog-pic/2019/2019-09-05-Fig-43-1-4.png)

## 43.7 举例：读取文件

读取文件时，LFS首先要读取CR区域，并将其缓存起来。然后给定一个节点编号就可以查到节点的硬盘地址；然后通过节点中保存的信息可以找到对应的数据区块。

## 43.8 目录怎么处理

上述讨论都是基于一个简化：只有文件而没有目录。那么有目录会是怎样的？

LFS与普通的文件系统一样，在目录数据中只保存了一些基本信息。因此在创建文件的时候，也要同时更新文件所在的目录。

![Figure 43.1.5](../../../tech-blog-pic/2019/2019-09-05-Fig-43-1-5.png)

一次文件访问流程如下：首先找目录，用目录的节点编号，从imap中找到目录节点地址，然后目录节点指向目录数据块；目录数据块可以告诉你文件的节点编号，拿着这个文件的节点编号，回到imap中找到文件节点地址，然后文件节点指向文件数据块。

节点地图同时还解决了另一个问题：`递归更新问题recursive update problem`。这个问题发生在类似LFS这样——只写新的而不更新旧的块——的文件系统中。比如，当更新一个节点的时候，要更新所在目录的节点，然后更新上级、上上级……一直到根部。

而LFS通过节点地图来避免了这个问题。想一下，即使节点的地址变了，但是节点中的信息并没有变，因此只改变节点地图就解决问题了，而不需要回溯上级。

## 43.9 新问题：垃圾回收

因为LFS并不会原地更新，所以当新的数据写入之后，老的数据依然会存在。

因此，LFS必须周期性的寻找那些老的数据、节点和其他数据体，并且`清除clean`他们。这个过程算是某种形势上的`垃圾回收garbage collection`，一种来自于编程语言的思想。

如果只是简单的将垃圾标记并释放，这会让硬盘中间中存在着大量的`可用空洞free hole`；等到复用这些区块的时候，就不能实现连续写入了，这样对性能会有显著的影响。

LFS采用一种逐段清理的方式，以此保证连续写入的性能。它是这样工作的：周期性地启动，每次读取数个区段，扫描其中存活的块，并将其连续写入到一个新的区段中。比如我们读取M个区段，然后重新写入N个区段，其中N<M。

那么问题来了：LFS如何识别一个区块是否存活？多久执行一次这样的垃圾回收？

## 43.10 机制：判断区块存活

LFS在每个`区段segment`中为每个区块保留了一些冗余的信息，包括每个区块的节点编号（属于哪个文件）以及偏离值（文件中的哪个块）。这部分信息保存在每个区段的头部，称为`区段总结块segment summary block`。

这样就很容易了。对每个区块，去找到它所属的节点，看下这个节点是否还指向这个区块。

利用这种机制，删除文件也变得非常简单。

## 43.11 政策：清除哪个块？何时清除？

决定何时是很简单，可以是周期性，可以是空闲时，也可以是硬盘快要装满的时候。

但是决定清理哪一块是很难的，也是目前业内的一个研究方向。
一种思路是给区段标记冷/热。热区段就是更频繁被覆盖的，冷区段就是最近更少操作的（即可能有一些死亡的区块和一些相对需要保留的区块），研究者的建议是优先清除冷区段。

但是绝大多数政策都不完美。

## 43.12 故障恢复与日志

当LFS正在向硬盘中写入时发生故障了会咋样？

（译者注：不感兴趣了，跳过）

## 43.13 小结

LFS的核心思想就是不替换掉原有的数据，任何更新操作都写在新的空间中。这种思路在数据库系统中称为`影子分页shadow paging`，在文件系统中叫`写时复制copy-on-write`。

最大的好处就是显著提升写入的性能，即使是随机写入也能获得连续写入的性能。
