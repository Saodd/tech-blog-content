```yaml lw-blog-meta
title: "flv.js核心思路解读"
date: "2022-12-10"
brev: "带你了解FLV协议，以及flv.js是如何处理flv资源的"
tags: ["前端","音视频","源码"]
description: "带你了解FLV协议，以及flv.js是如何处理flv资源的"
keywords: "flv.js"
```

## 背景

在web端上实现音视频相关的功能，我们往往总是难以绕开『FLV』这个话题。

今年10月-11月，大概两个月的时间里，我系统性地调研了前端音视频的相关技术。包括：

- 阅读`FLV`、`MP4`规范，自己手写了 flv-mp4 格式转换器，并分别以`golang`和`js`两个语言进行实现。
- 以`WebCodecs`为核心，在浏览器中实现了帧级的解码、播放、编辑等操作。
- 参考`flv.js`，自己手写了浏览器flv播放器。
- 在浏览器中引入`ffmpeg.wasm`（[《ffmpeg.wasm 踩坑体验》](../2022/221028-ffmpeg-wasm.md)）

在这次调研过程中，接触了很多 Web API ，多到写几篇博客都写不完的程度。这次我再以`flv.js`这个维度做个剖面，带大家了解一下其中的一些核心原理。

## flv.js的一些调试技巧

[Bilibili/flv.js](https://github.com/Bilibili/flv.js/)是一个开源的、支持浏览器中播放flv流的库，它已经是最流行的flv解决方案之一了（虽然我对这个项目的代码组织架构有许多抱怨）。

在本地调试时，只需要普通地使用`yarn link`能力，然后在`flv.js`源码目录里执行`run dev`命令即可，没有任何的坑。

### 视频分片读取

一个视频可能很大、很长，例如几个小时的直播录屏的体积可能会有几个GB 。而当用户在浏览器中浏览视频的时候，我们不需要加载整个视频文件，而是只取出前面一小段，就可以开始播放了；等快播放完了这一段，再取下一段。

要实现这个能力有多种解决方案。其中一种是借助 [HTTP range requests](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests) 能力，这个特性需要服务端的支持。我们的资源在云OSS上，云厂商已经默认提供了这种能力，因此只需要操心前端实现即可。

在`flv.js`中使用时，需要侵入`RangeSeekHandler.getConfig()`源码中去修改头部，即可下载指定的一小段二进制数据。

### 导出经过转换后的视频

flv.js 的视频播放器用的是 MSE(MediaSource) ，这个东西呢，丢进去的视频数据是[无法再吐出来的](https://stackoverflow.com/questions/50938089/how-to-download-or-get-as-a-blob-a-mediasource-used-for-streaming-html5-video)。因此如果我们需要把视频导出来用其他工具来分析，就要侵入源码，在每次`sourceBuffer.appendBuffer`之前把数据给拷贝出来。

示例代码如下：

```js
class MSEController {
    _doAppendSegments() {
        //...
        if (type === "audio") {  // or type==="video"
            myBuf.set(new Uint8Array(segment.data), myBufCount)
            myBufCount += segment.data.byteLength
        }
        // this._sourceBuffers[type].appendBuffer(segment.data);
        // ...
    }
}
const myBuf = new Uint8Array(new ArrayBuffer(1024*1024));  // 因为不能append操作，因此要提前分配足够的内存空间
let myBufCount = 0;
```

解释一下，因为一个`MediaSource`里可以有多个`SourceBuffer`，而`flv.js`这个库的处理是将音频和视频分开，分别存入一个SourceBuffer中去。因此最后我们得到的也会是两个`mp4`文件，分别仅包含音频和视频数据。

我们得到了二进制数据之后，使用`a`标签的下载能力来将它导出为文件，便于后续的研究。

```js
class MSEController{
    _onSourceEnded() {
        // fired on endOfStream
        // Log.v(this.TAG, 'MediaSource onSourceEnded');

        const link = document.createElement('a');
        // 注意Blob的参数要以数组传入，并且传入的应该是ArrayBuffer而不是Uint8Array
        link.href = URL.createObjectURL(new Blob([myBuf.slice(0, myBufCount).buffer]));  
        link.download = 'xxxxxx.mp4';
        document.body.appendChild(link);
        link.click();
    }
}
```

### 检视二进制数据

有一些专用工具可以直接打开一个二进制文件（flv, mp4等），让你能够逐个字节地进行观察。

我没有找到好用、开源、免费的PC端软件，但是在Jetbrains的插件中找到一个叫做`BinEd`的免费插件，它已经足够好用了（虽然每次会报错，但是能正常使用）。

再用自己的代码对flv和mp4做结构化解析、并将文件结构以JSON形式导出，双管齐下，可以支持任何的开发调试需求了。或者用开源的工具，例如[bento4](https://www.bento4.com/)也可以检查MP4结构化数据。

### 寻找各种ISO标准文件

音视频编码这个领域，规范非常的多；哪怕就只说`mp4`这一个东西，它也是由很多个ISO规范组成的一个大家族。

既然是"ISO"，我认为就自然应该向公众免费提供。然而现实是：免费提供的只有过时的版本，而最新的版本都是要收费下载的（而且贵得离谱）。google上也很难找到盗版资源。不过毕竟我们不是专业做这个的，而且考虑到兼容性，其实用免费的旧的规范已经足够了。

最大的难点：规范都是全英文的，需要相当扎实的英语阅读能力才能顺利啃下来。

## FLV

### 规范

FLV 是 ADOBE Flash 的数据格式，虽然『Flash播放器』已经死了，但是『FLV』这种编码格式，则由于历史原因和它的某些优秀特性，依然活跃在互联网中。

规范文件： [Adobe Flash Video File Format Specification V10.1](http://download.macromedia.com/f4v/video_file_format_spec_v10_1.pdf)

### 为什么视频网站选择flv作为视频格式

参考[又拍云的回答 - 知乎](https://www.zhihu.com/question/26656502)，有两点优势：

1. 兼容目前的直播方案：目前大多数直播方案的音视频服务都是采用FLV容器格式传输音视频数据。
2. FLV容器格式相比于MP4格式更加简单，解析起来更快更方便。

所谓“容器”，通俗来讲就是flv封装数据的格式。

一个flv文件，首先有一小段头部(`FLV Header`)，剩下都是一个个独立的数据包(`FLV Packet`或者叫`FLV Tag`)，这种结构非常适合网络传输。

### FLV Header

参考：[Flash Video - wikipedia](https://en.wikipedia.org/wiki/Flash_Video) 或者 [fileformat - flv](https://docs.fileformat.com/video/flv/)

配合[flv.js](https://github.com/bilibili/flv.js/blob/master/src/demux/flv-demuxer.js#L133)的源码，先看看头部：

```js
function probe(buffer) {
    let data = new Uint8Array(buffer);
    let mismatch = {match: false};

    // 前3字节是 'FLV' 三个字符
    // 第4字节是版本号，只有 0x01 这一个合法值
    if (data[0] !== 0x46 || data[1] !== 0x4C || data[2] !== 0x56 || data[3] !== 0x01) {
        return mismatch;
    }

    // 第5字节是Flags ，用于鉴别是否含有音频、视频数据
    let hasAudio = ((data[4] & 4) >>> 2) !== 0;
    let hasVideo = (data[4] & 1) !== 0;

    // 6-9字节，组成一个int32是 Header Size，这个值一定不小于0x09
    let offset = ReadBig32(data, 5);
    if (offset < 9) {
        return mismatch;
    }

    return {
        match: true,
        // ...
    };
}
```

简而言之，前9个byte是属于`flv`的头部，后续跟着的都是数据包了。

头部的长度至少会是9，但是一般也就是9，因为flv的格式已经没有大的变化了。

### FLV Tags

每个`Tag`都有固定长度 15bytes 的头部。

![FLVPacket](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/FLVPacket.png/547px-FLVPacket.png)

第一个部分是`Size of previous packet`。它是上一个packet的大小，但是注意，计算方式是`payload`的长度加11字节的头部，也就是说，它本身这4个字节是不包含在计算之内的。（从这个角度来说，“15字节的头部”应该理解为“11字节的头部 + 4字节的尾部”才更加合适。）这个字段的作用是帮助从后向前定位。

第二个部分，1个字节是`Type`，表示数据包的类型。常见值`18`是脚本数据（或者称为Meta数据），`9`是视频数据，`8`是音频数据。

第三个部分，3个字节的数字表示`Payload Size`，即头部之后的数据体的大小。

第四个部分，4个字节组成`Timestamp`，这个时间戳的值是从视频开始计算的毫秒，而不是Unix时间戳。而且由于“关键帧”的存在，不同的Packet之间的时间戳可以相隔很大，整个FLV依然能够正常使用。另外注意它的字节顺序很乱，需要特殊处理。

第五部分，3个字节的数字表示一个`Stream ID`，通俗理解就是轨道辨识符。多个Packet虽然可以在字节上不连续，但是通过这个ID来组成一个逻辑上连续的时间轨道。

参考代码：

```ts
// 这里传入的一个"Chunk"，可能会包含多个 FLV Packets
function parseChunk(buf: ArrayBuffer) {
    let offset = 0;
    while (offset < buf.byteLength) {
        const view = new DataView(buf, offset);
        if (view.byteLength < 15) {
            console.log(`剩余${view.byteLength}字节不能解析`);
            break;
        }

        const prevPacketSize = view.getUint32(0, false);
        const packetType = view.getUint8(4);
        const payloadSize = view.getUint32(4, false) & 0x00ffffff;
        const timestamp = _getPacketTimestamp(view);
        const streamId = view.getUint32(11, false) & 0x00ffffff;  // 12-14共计3字节，向前取一字节后丢弃

        console.log({prevPacketSize, packetType, payloadSize, timestamp, streamId});
        offset += 15 + payloadSize;
    }
}
```

### Data Tag

上面说的“脚本数据”，或者确切地说，是`Type=18`时的数据包。

一个FLV文件的第一个Packet往往都是这样的metadata的包，它可能包含如下信息：

- `duration`：视频持续时间，float64秒数。对于直播流来说，可能会写为"0"。
- `width`和`height`：视频的像素宽高，float64
- `framerate`：帧率，float64
- `keyframes`：关键帧的位置，数组

上面的数据，如果在支持脚本的Flash播放器中，会作为参数被传递给一个名为`onMetaData`的回调函数。（所以flv.js解析出来的结构体是`{onMetaData:{...}}`）

在JS中的处理可以参考[flv.js/amf-parser](https://github.com/bilibili/flv.js/blob/master/src/demux/amf-parser.js)，用它解析一个抖音直播视频，会得到如下的值：

```js
onMetaData = {
    duration: 0,
    width: 540,
    height: 960,
    framerate: 25,
    videocodecid: 7,  // MP4 H.264
    // ...
}
```

### Audio Tag

`Type=8`是音频数据。它的`payload`部分的第一个字节用来记载编码信息，其中前4bit是编码格式，后4bit是编码参数。

`flv.js`支持的编码器格式有两种，`2`（MP3）和`10`（MP4 AAC） ；在抖音视频中还会见到`1`（ADPCM）的数据包，可以直接丢弃掉。

### Video Tag

`Type=9`是视频（图像）数据。它的第1个字节记录了这一包的类型（是否关键帧），以及编码器ID（例如AVC）。

`flv.js`仅支持解析AVC格式。

AVC Tag 又分为两种，一种是记录了编码相关配置信息（AVC Decoder Configuration Record），另一种则是纯数据包。一般来说配置包必须出现在数据包之前，并且只出现一次。

## flv.js解读

### 宏观结构

- player: 外部操作接口，并且负责与`HTMLMediaElement`进行绑定。
- core: 核心部分，
  + transmux: 负责数据流动的中转控制。
  + mse-controller: 负责`MediaSource`相关操作。
- io: 负责从网络中加载数据，支持多种协议。
- demux: 将下载来的数据流进行解析
- remux: 将解析后的音视频数据转化为mp4格式（因为浏览器仅支持这种格式）

官方给出的标准用法如下：

```js
if (flvjs.isSupported()) {
    var videoElement = document.getElementById('videoElement');
    var flvPlayer = flvjs.createPlayer({
        type: 'flv',
        url: 'http://example.com/flv/video.flv'
    });
    flvPlayer.attachMediaElement(videoElement);
    flvPlayer.load();
    flvPlayer.play();
}
```

下面分步骤来看看都发生了一些什么事情：

### 第1步：初始化

```js
var flvPlayer = flvjs.createPlayer({
        type: 'flv',
        url: 'http://example.com/flv/video.flv'
    });
```

这一步会返回一个`new FlvPlayer()`，只做了一些对象属性初始化的动作。源码：[src/player/flv-player.js](https://github.com/bilibili/flv.js/blob/master/src/player/flv-player.js)

### 第2步：绑定video元素

```js
flvPlayer.attachMediaElement(videoElement);
```

这一步需要传入一个`HTMLMediaElement`（或者说就是一个`video`标签元素），然后它会在这个元素上监听`loadedmetadata`, `seeking`, `canplay`, `stalled`, `progress`等五个事件。

然后创建一个`new MSEController()`。

`mse`会与`player`进行互动，例如当`mse`解析完成之后会通知`player`考虑要不要暂停后续的帧解析动作。

`mse`创建`MediaSource`之后将其转化为`createObjectURL`并设置到`HTMLMediaElement`上去。

### 第3.1步：加载数据

```js
flvPlayer.load();
```

这一步会创建一个`new Transmuxer()`。它在初始化的时候会尝试在`WebWorker`中运行那些二进制数据处理逻辑，也就是`new TransmuxingController()`。

`transmuxer`也与`player`有大量的互动，互动都是通过`EventEmitter`来传递的。例如，当`transmuxer`说视频资源已经加载完毕后，会通知`player`，也会通知`mse`去做`MediaSource.endOfStream()`的动作。

当执行`transmuxer.open()`的时候，最终实际执行的是`TransmuxingController._loadSegment(0)`，也就是加载视频流的第0段数据。

每次加载的时候都会创建一个`new IOController()`来控制进度以及处理事件，它里面有`loader`和`seekHandler`，这两个东西都是允许开发者从外部自定义的，如果使用默认值则可能会有`new RangeSeekHandler()`和`new FetchStreamLoader()`。

获取到的数据最终会通过`_onDataArrival`传回`IOController`去处理。具体加载的逻辑比较复杂，因为在处理的时候是具体到字节的，因此每次加载可能都会多余一部分要等待与下一段数据进行拼接再使用，这里不展开讲。

然后又通过同名的`onDataArrival`传回`transmuxer`去处理，它就做两件事：让`demuxer`解析二进制数据，然后交给`remuxer`组装成MP4格式然后拿去`mse`那里进行播放。（这部分的代码全部都是通过事件处理函数来进行传递的，阅读难度很大，建议不要死读，要结合宏观层面的抽象概念一起理解。）

### 第3.2步：demux

`demuxer.parseChunks()`中拿到了刚刚下载来的（flv格式）二进制数据。

它根据FLV格式标准（本文前一章节中介绍的）对二进制数据进行解析，得到了一个一个的`Tag`，然后根据数据的类型分别传给不同的函数进行进一步处理：

```js
switch (tagType) {
    case 8:  // Audio
        this._parseAudioData(chunk, dataOffset, dataSize, timestamp);
        break;
    case 9:  // Video
        this._parseVideoData(chunk, dataOffset, dataSize, timestamp, byteStart + offset);
        break;
    case 18:  // ScriptDataObject
        this._parseScriptData(chunk, dataOffset, dataSize);
        break;
}
```

在每个parse函数的最后，都会将它解析出来的结果传出去。例如在`_parseAVCDecoderConfigurationRecord()`函数中最后就调用了`this._onTrackMetadata('video', meta);`，将数据传递给`MP4Remuxer._onTrackMetadataReceived()`进行处理。

### 第3.3步：remux

`remuxer`会将前面解析出的结构化的数据，封装为MP4格式。例如前面提到的`_onTrackMetadataReceived()`函数，其核心逻辑可以简化为：

```ts
class MP4Remuxer {
    _onTrackMetadataReceived(type, metadata) {
        metabox: Uint8Array = MP4.generateInitSegment(metadata);
        this._onInitSegment(type, {
            data: metabox.buffer,
        });
    }
}
```

由于FLV与MP4两种格式的组织方式有根本性的不同，因此上面看起来简简单单的`generateInitSegment()`函数的一次调用，背后其实隐含着一大坨代码，默默地干了大量的脏活累活。这里不展开讲，也许等以后想讲MP4格式的时候可以涉及到。

最后这份MP4数据会（可能经过WebWorker的传递）最终回到`mse`那里去，然后被注入浏览器的播放器中进行解析、播放。

### 第3.4步：mse

接着上一节的内容，`mse.appendInitSegment()`会收到前面传来的`generateInitSegment()`所生成的MP4格式的二进制数据包。

顾名思义，这是个『init』数据包，因此对它的处理方式与后续音视频数据包的处理方式略有不同。

它首先要做的事情是在`MediaSource`上创建一个`SourceBuffer`，音频、视频各建立一个。

`SourceBuffer`这个东西非常不好操作，它的所有解析动作都是异步的，而且还不接受队列、一次只能解析一份MP4数据包，要想连续处理多个片段，只能我们自己监听事件来进行处理。这块也不展开讲，总之呢，核心方法就是`SourceBuffer.appendBuffer()`。

### 第5步：播放

当音频、视频数据分别被`MediaSource`解析完毕之后，video标签元素上转圈圈的loading状态就会消失了，同时control控制条上的时间轴也逐渐被填充、拉长。

```js
flvPlayer.play();
```

就是简单地控制video元素开始播放。至此，『从头播放一个视频』的能力已经被完整实现了。

### 跳转播放：seek

当用户点击video标签上的时间轴的时候，也就是想要跳转到指定的时间继续播放的时候，会触发`seeking`事件并被`player`捕捉到，进入`FlvPlayer._onvSeeking()`函数进行处理。

首先它会看看指定的时间是否已经被缓存在`MediaSource`里，如果是，那就直接让浏览器跳转就可以了。

否则，它要通知`this._msectl.seek()`和`this._transmuxer.seek()`，最后与3.1步加载数据相似，都要走到`transmuxer._loadSegment()`，使用`IOController`来加载数据然后交给`demuxer`和`remuxer`。

## 小结

flv由于其格式简单以及历史原因，一直广泛活跃在当今互联网上。我在调研这块的时候我一直在想，可为什么浏览器就不能兼容flv格式呢？如果说Flash有安全隐患的话，那哪怕浏览器只支持flv的一个子集，能支持基本的音视频播放，也是好的啊。

像`flv.js`这样一通操作，做了太多脏活累活，可其实从效率的角度来评价的话，只是在绕远路罢了。简简单单“音视频”三个字，背后真的蕴含了太多太多东西，也暗藏着太多的商业利益纠葛。着实让人无奈。

就个人而言，我在深入研究音视频相关技术的时候，收获还是很大的。在这个领域，很多实现方式是与典型的、以状态驱动的web前端开发的理念是完全不同的，虽然我说`flv.js`的代码实现有很多地方我都想吐槽，但同时我也从中学习到了一些新的代码设计思路，这点我还是非常感激的。
