```yaml lw-blog-meta
title: "Electron的一种用途"
date: "2022-09-18"
brev: "本文仅供交流学习使用：）"
tags: ["前端"]
```

## 背景

我第一年做Python开发的时候，我就已经尝试过了所谓爬虫。不过当时玩得比较简单，大概就是定时爬取某些网站的内容，多一点就会做个正则匹配、做翻页、下载附件之类的。

我的大概印象，当时主要的痛点有三个：

1. 一个是用正则去解析HTML有些蛋疼，一个简单的DOM我要写很复杂的正则才能取出东西来（当然这个问题可以用`XML`解析库来帮忙解决）；
2. 登录态、风控机制难以应对
3. js逻辑没办法处理，例如js翻页、点击事件等

后来在项目后端服务中，我搭了一个`playwright`服务，虽然它的目的不是爬虫，但是这次开发经历给了我不小的启发。

再到最近手头的项目，我研究`Electron`研究了将近一个月，已经把它的核心用法都玩熟练了。

于是现在“手里拿着锤子”，遇到“钉子”的时候，毫不犹豫就想上去锤两下——这次我尝试用Electron来爬取一个盗版小说网站。

为了避免纠纷，在本文中我隐藏了一些敏感信息。这个被我爬取的盗版小说网站，就称它为`a.com`吧。

## Electron基本用法

来，先学一下“1+1=2”：

```js
async function createWindow() {
    const window = new BrowserWindow()
    // 拿window可以搞事情了
}

app.whenReady().then(() => {
    createWindow()
})
```

## 获取页面内容

首先用肉眼观察一下这个网站的页面结构，看起来是一个很典型的`jQuery`页面，它的HTML上用了很多的`id`，极大地方便了我的爬取。

### 获取小说文本

在小说文本区域有一个父容器，它有一个非常显眼的`id="acontent"`，用这个就能把内容取出来，代码如下：

```js
const resp = await window.webContents.executeJavaScript(
    `document.querySelector('#acontent').innerHTML;`
);
fs.writeFile('1.html', resp, (err)=>{});
```

其实，因为它页面上有很多广告，我一开始的思路是用`textContent`，它可以取出更加纯净的文本内容；但是它处理不了插图，因此我才选用了`innerHTML`。而其实后者的效果更好，因为原始的HTML中是并没有广告的，广告的DOM是在js执行之后才插入进去的。

拿到HTML文件后，手动打开一下，展现效果很完美。

### 获取图片内容

在上面获得的HTML中有很多`<img >`标签，它们依然是指向原始地址的。而我希望把这些图片也都保存到本地。

观察一下图片的特征，它们都是放在一个单独的域名下的（`img.a.com`）。

那么我只需要监控这个页面的所有请求，（img标签加载资源也是一种请求），在完成某个请求后会触发相应的生命周期事件，我捕获它们即可：

```js
session.webRequest.onCompleted({urls:['https://img.a.com/*']}, async (details) => {
    // ...
})
```

但是这里有个小问题，虽然是`onCompleted`的事件，但是在这里是取不到响应内容的。因此我需要再次从页面中去取。

一个简单粗暴的思路就是再次调用`fetch()`，所幸，所有图片资源都设置了缓存，因此我再次fetch的开销可以忽略不计：

```js
const resp = await window.webContents.executeJavaScript(
    `fetch('${detail.url}').then(resp=>resp.arrayBuffer())`
);
fs.writeFile('1.jpg', new DataView(resp), (err)=>{});
```

### 用本地图片替换

上一步中我们已经把图片保存到本地了，接下来我们需要修改HTML中的内容，把原来`img.a.com`的图片替换到本地的`localhost:xxxx`上去（用相对路径表示）

说起来简单，但是这一步要处理好先后执行关系，是需要对js的Promise运行机制非常熟练才能写出来的。

简而言之，在每个页面`loadUrl()`执行完毕之前，所有图片的请求一定都会`onCompleted`，因此我们需要在`onCompleted`里面注入一些能够让主协程停下来的逻辑，要等待所有图片都保存到本地之后，才能继续下一步，即替换HTML中的文本并且保存HTML文件。

（代码改造太大，省略。）

值得一提的是，node.js的`fs`库都是同步版本的API，因此为了我们的代码逻辑清晰，在每次调用的时候都要手动`new Promise()`处理，将同步改为异步代码。

### 阻止无关的请求和代码运行

从上面的步骤可以看出，我只需要最初的HTML请求，然后HTML中会携带所有的图片地址，因此除了它们之外的所有资源我都屏蔽掉即可。

```js
session.webRequest.onBeforeRequest((details, callback)=> {
    if (details.url.startsWith('http')) {
        if (
            details.url.startsWith('https://a.com/novel') || 
            details.url.startsWith('https://img.a.com/')
        ) {
            callback({});
        } else {
            callback({cancel: true});
        }
    } else {
        callback({});
    }
})
```

## 形成任务队列

首先我们要通过观察页面结构，来从当前页面上获取到下一个页面的地址，这样我们的爬虫程序才能连续运行。

然后就可以设计我们的队列系统了。

由于JS是单线程运行的，因此只要正确处理好Promise协程之间的先后关系，那么我们可以用一个简单的数组来当作队列使用。核心代码如下：

```js
const urlPathQueue = ['/novel/1.html']  // 起始的第一个页面

async function runTask(urlPath) {
    // ...处理一个页面
    urlPathQueue.push(/* 加入下一个任务的url */)
}

async function main(){
    for (let i = 0; urlQueue.length; ++i) {
        await runTask(urlQueue.shift())
    }
}
```

### 多页内容拼接为一页

这个小说网站，它为了提升点击量（靠广告盈利）、同时也是避免某些太长的章节，它把原小说一个章节的内容会拆分为多个HTML页面。

而我们爬取下来使用，当然要提升自己的阅读体验，因此要做拼接处理。

一个简单的处理是，根据章节id来生成文件名，然后如果遇到同一个章节的多个页面，则可以判断本地文件是否存在的方式来进行拼接。

### 终止条件

观察页面结构，可以发现这个网站在小说的最后一页，它的“下一页”指向的是“目录页”。

此简单考虑，我们可以直接写死，用这个目录页的地址来进行判断。

### 构建目录

在形成队列的时候，我们需要一个顺序，同时也需要一个目录。

这个东西是要求有序、去重的，因此直接用js的`Map`数据结构即可天然保证这两点。

在程序执行完毕（所有页面爬取结束）之后，我们将这个Map中的数据序列化到一个`.json`文件中去，这样我们刚才下载下来的HTML就能够通过这个json文件有序地组织起来了。

等爬取到全部数据之后，拿这个json里的目录数据，生成一个目录页，然后给目录页和章节页分别添加一些样式，以及“下一页”的按钮。就大功告成了。

## 没有做的优化

至此为止，我这套爬虫程序已经具备了基本的能力了。

于是我尝试运行，于是……它一直正常地运行了一个小时，运行到了正常结束，让我既意外又欣慰。

在试运行之前，其实我很担心它会不会有什么反爬手段。

虽然在页面上简单地观察到了一些初级的反爬措施，不过我以力破巧，简单粗暴地禁用一切js和css，只看HTML，这样的话管他什么措施都是拦不住我的。而这个网站坚持把小说内容放在HTML中，其实用意也很容易猜到，就是为了提升SEO效果。

其实在我的想象中，正经的反爬措施应该在后端实现。例如埋一些风控参数在Cookie中或者其他Header字段中（很多大厂产品都这么做），例如简单地加个IP访问频率限制，例如执行一些js逻辑，例如增加一些随机异常或者验证码，例如检查referer……

哪怕就是在前端做措施，也可以有不少手段来增加爬虫开发者的心智负担。

如果他真的这么干，我相信一定可以给我增加许多麻烦，至少不会像今天这样简单，简单3个小时就完成了整个开发过程+博客写作过程。

但是相反，他整个网站都做得很干净，代码也写的挺漂亮，甚至就连网络访问速度都相当优秀，甚至它还做了安卓和ios的客户端，客户端体积也超小，真的可以算是盗版网站中的楷模了，泪目。

## 小结

其实这篇文章所讲的技术，是完全可以拿去做灰色产业的。所以我故意没有讲得很详细，只是记录一个大概的思路，就连标题也起得含糊，生怕被Google收录然后被哪个愣头青拿去干些不得了的事情。

再次强调，本文目的仅仅讨论Electron相关技术的使用，请勿滥用这项技术。

技术本身并没有善恶，使用技术的人才分善恶。
