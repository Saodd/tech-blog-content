```yaml lw-blog-meta
title: 'k8s 入坑日记'
date: "2021-02-06"
brev: "该学的终归还是要学"
tags: ["运维"]
```

## 前言

虽然说「术业有专攻」，但是就像后端程序员懂点前端知识会有利于开展工作一样，后端程序员同样也需要了解运维相关的知识。而运维的极致自然是`k8s`。

回想我的个人经历，早在2019年5月，我才刚刚入行IT不到半年、连Linux常用命令都敲不出几个的时候，我就开始在 [折腾Docker](../2019/190509-使用Docker部署MongoDB.md) 了。随后很快，在当月，我就引入 Docker swarm 来维护我这个个人博客网站。但我一直很清楚`swarm`只是个玩具，在2020年初，我尝试折腾了一下`k8s`，照着官方教程走了一遍minikube，但是发现学得很痛苦，学完之后在本地环境依然折腾不起来，遂暂时作罢。

此后过了整整一年，在进一步熟悉 Docker 和 swarm 的基本操作，从[etcd](../2021/210116-etcd-guide.md) 了解分布式和服务发现理论，从[caddy](../2021/210127-caddy-gin-jwt.md) 和 [Envoy](../2021/210129-learn-envoy.md) 了解网络代理，之后，我再次学习 [minikube](https://kubernetes.io/zh/docs/tutorials/) 我发现此时的我已经进入了一个新的境界，我开始真正地理解那些晦涩的概念。我想，我该进入下一个阶段了。

> 本文用到的代码托管在 [Github](https://github.com/Saodd/learn-k8s)

## I. 关于minikube

其实对于单机运行k8s这件事，业内有三个解决方案：minikube, kind, k3s. 它们之间的区别可以参考：[Minikube vs. kind vs. k3s - What should I use?](https://brennerm.github.io/posts/minikube-vs-kind-vs-k3s.html)

[minikube入门指南](https://minikube.sigs.k8s.io/docs/start/)

我整个操作一遍下来，感觉稍微有一些坑，但是总体还算顺利。所以虽然我已经总结了关于minikube的流程，但是想想觉得没什么意思，还是不放出来了，删了。

下面直接开始搞k8s吧。

> 强烈建议初学者先去了解k8s的各种概念，例如 Pod, Deployment, Service, Node 之类的，否则可能会导致虽然安装成功但是仍然不知道自己在做什么的情况。如果发现还是看不懂这些概念，建议像我一样，先从 Docker swarm 开始玩起。

## II. 安装kubeadm

参考： [k8s文档 - 安装 kubeadm](https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)

这个过程中你可能需要注意：

### 1. 配置网络代理

因为k8s背后的推动者是Google，所以有很多资源都是不存在的，我们需要一点点特殊的办法（自己解决），然后在终端中配置环境变量：

```shell
export PROXY=http://xx.xx.xx.xx:10809
export ALL_PROXY=$PROXY HTTP_PROXY=$PROXY HTTPS_PROXY=$PROXY
```

注意`apt-get`是用不了代理的，所以我们需要：

```shell
sudo vim /etc/apt/apt.conf.d/proxy.conf
```

写入（记得安装完成之后要删除掉哦不然下次apt的时候还会走代理）：

```text
Acquire {
  HTTP::proxy "http://127.0.0.1:1082";
  HTTPS::proxy "http://127.0.0.1:1082";
}
```

然后发现，`grc`的镜像实在是，vxxx代理都解决不了，所以我这里用最鱼唇的办法，从本地机器上`docker save`上传然后`docker load`...

### 2. 禁用swap. 

参考 [Ubuntu 16.04 禁用启用虚拟内存swap](https://blog.csdn.net/CSDN_duomaomao/article/details/75142769)

### 3. 用systemd代替cgroupfs. 

参考 [stackoverflow](https://stackoverflow.com/questions/43794169/docker-change-cgroup-driver-to-systemd)

编辑`/etc/docker/daemon.json`:

```json
{
  "exec-opts": ["native.cgroupdriver=systemd"]
}
```

然后重启Docker:

```shell
sudo systemctl restart docker
```

## III. 创建集群

参考： [k8s文档 - 使用 kubeadm 创建集群](https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)

简单点，不用搞什么参数了：

```shell
sudo kubeadm init
```

> 如果不小心搞错了什么，可以用 kubeadm reset 命令撤销已经初始化的状态。

这个过程中，会拉取google的docker镜像，所以一定需要正确的网络连接。

最后会输出如下内容：

```text
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.6.239:6443 --token cnupff.xxxxxxxx \
    --discovery-token-ca-cert-hash sha256:xxxxxxxxxxxx
```

按上面的提示内容操作一番吧，把配置文件保存在当前用户的目录下。

```shell
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

此时先避开一个坑。k8s集群是通过6443端口来提供服务的，因此如果我们前面配置了代理，并且代理是全部转发的话，我们的本地客户端联系k8s的请求也会被转发到代理上去，然后很自然地找不到地址了。所以我们要把本地地址配置为不走代理，通过一个环境变量：

```shell
export NO_PROXY=10.0.6.239  # 你的IP地址，或者去看看代理软件的日志
```

此时整个集群还是不可用的，要先配置网络组件。虽然有很多选择，但官方推荐的是Calico. 进入它的官网会发现依然很迷茫，在左侧导航列表里，看起来只有 Install Calico -> [Self-managed on-premises](https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises) 这个选项卡是符合我现在的情况的（在现场安装而非云上）（也许选错了，请指正）。

根据k8s的意思，安装网络组件其实也就是`kubectl apply xxx.yaml` 一条命令的事情。然后看看Calico的安装教程，果然如此，我们按照流程操作：

```shell
curl https://docs.projectcalico.org/manifests/calico.yaml -O
kubectl apply -f calico.yaml
```

然后我们可以通过`kubectl get pods --all-namespaces`命令来检查，可以看到当前创建了几个新的pod，正在初始化。

请稍等片刻。如果担心有问题，可以用`kubectl describe pod calico-node-dq5hb -n kube-system`这类命令去检查 Pod 的 event 。

此时，我们就创建起了一个k8s集群，现在其中只有一个节点，而且是控制节点（或者叫控制平面）。

默认情况下不会在控制平面节点上调度Pod，如果需要的话（单机k8s则必须开启此项），执行：

```shell
kubectl taint nodes --all node-role.kubernetes.io/master-

# 输出内容
node/your-hostname untainted
```

如果你有其他的主机，想让它们作为工作节点加入集群，执行（其实就是刚才init输出内容的最后一行）：

```shell
sudo kubeadm join --token <token> <control-plane-host>:<control-plane-port> --discovery-token-ca-cert-hash sha256:<hash>
```

token有效期24小时，需要一些命令去重新生成。详情请直接参考官方文档。

然后强调一下，现在是只有一个控制节点的集群，并不是高可用的。如果需要高可用，一般是要3个控制节点，详情请参考官方文档，我这里不再深究了。

然后再再提一下，需要Dashboard的同学可以参考 [dashboard文档](https://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/README.md)

然后再再再提一下，由于kubectl会是一个非常常用的命令，所以可以考虑`alias k=kubectl`，甚至直接放入`/etc/profile`里面去。

然后再再再再提一下，Jetbrains 家有 kubernetes 的插件，我试用了一下觉得很强大。

## IV. 创建应用 (Deployment)

我们以一个最简单的、无需任何依赖的应用，Nginx，作为例子。参考 [k8s文档 - 使用Deployment运行一个无状态应用](https://kubernetes.io/zh/docs/tasks/run-application/run-stateless-application-deployment/)

这里先提一下k8s的理念：「一切配置皆是文件」。我的理解是，所有的配置都不在运行时手动操作，而是都应该固化为代码文件。作为文件有很多好处，其中最重要的一点就是可以通过git等代码管理工具进行管理，并且容易追溯、容易复制、容易迁移。

要启动一个应用，首先从编写配置文件开始。我们之前应当很熟悉 docker-compose（或者swarm）的`.yaml`文件的编写方式了，这里k8s的配置文件是类似原理，只不过是语法不同罢了。

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.18.0-alpine
          ports:
            - containerPort: 80
```

上面的文件中写到：我们定义的是一个`Deployment`，这个deployment的名字叫`nginx-deployment`，其中每个Pod中包含一个容器，容器使用的镜像是`nginx:1.18.0-alpine`，暴露容器端口`80`（注意不是宿主机端口），并且给所有Pod注入`nginx`这个标签。

把这个配置文件上传到控制节点服务器，然后执行：

```shell
kubectl apply -f xxxx.yaml
```

接下来可以通过 describe 等命令观察应用的状态。

然后我们要怎样去访问到这个服务呢？

### 方案一：通过`NodePort`直接暴露节点端口

要分清概念，Deployment只是在集群内网内暴露了80端口，要在集群内网与外界之间建立联系，必须要通过 Service ：

```shell
kubectl expose deployment nginx-deployment --type=NodePort --port=80
```

然后我们找到在节点上（宿主机上）实际暴露的端口号：

```shell
$ kubectl get svc
NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
nginx-deployment   NodePort    10.108.205.134   <none>        80:30127/TCP   7s
```

得知是`30127`，于是我们可以从其他机器上从30127来访问这个服务，IP地址是宿主机的IP地址，注意不是上面这个Cluster-IP。

### 方案二：通过 port-forward 临时转发端口

理解为就是一个网络代理，运行在宿主机上，把外界端口转发到集群内网中去：

```shell
kubectl port-forward --address 0.0.0.0 deploy/nginx-deployment 9999:80
```

上面的命令将宿主机的`9999`端口转发到了 `deploy/nginx-deployment`的`80`端口上。可以通过浏览器去验证。不过要注意这是一个临时转发，关闭进程之后就消失了。

### 方案三：通过Ingress

这个应该是生产上的正确用法？但是我研究了一下，似乎对本地k8s集群支持很不友好，所以暂且不去深究。

不过这个过程中尝试了一下`Helm`，感觉还挺神奇的，后续有空研究一下，咕咕咕。

### 方案四：自定义`NodePort`

参考 [Kubernetes 调整 nodePort 端口范围 - csdn](https://blog.csdn.net/qianghaohao/article/details/99656364)

k8s默认是限制`NodePort`的可用端口的，一般是30000-32767。

如果我们想把web服务优雅地挂载在80等默认端口上，就需要一些额外的配置了。

配置方式是直接去修改`/etc/kubernetes/manifests/kube-apiserver.yaml`，在 command 下添加 `--service-node-port-range=1-65535` 参数。

然后我们尝试在之前 nginx-deployment 的配置文件中，再写一个 service 配置：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.18.0-alpine
          ports:
            - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-deployment
spec:
  ports:
    - port: 80
      nodePort: 80
      protocol: TCP
  selector:
    app: nginx
  type: NodePort
```

这里稍微解释一下，对k8s来说，不论是 Deployment, Pod, 还是 Service 这种抽象的东西，都是一种「资源」，它都可以定义在`yaml`文件中。

我们把上述文件上传到k8s控制节点上，apply一下，然后就可以通过80端口来访问服务啦。

不过还是要强调一下，`NodePort`这种模式不是为生产设计的。生产上应该使用`LoadBalancer`模式的Service。（如果不对请指正）

## V. 小作业：Sidecar模式运行应用

Pod 的最大特点是，Pod可以包含若干个容器，并且同一个Pod内的容器可以**共享资源**。最典型的就是通过localhost就可以互相访问。

我这里借用之前 [学习caddy的代码](https://github.com/Saodd/learn-caddy) 并稍加改造。

用从顶向下的思路来思考吧。先定义k8s资源：

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: learn-caddy
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: learn-caddy
    spec:
      containers:
        - name: learn-caddy-auth
          image: learn-caddy:auth
          ports:
            - containerPort: 30000
        - name: learn-caddy-business
          image: learn-caddy:business
          ports:
            - containerPort: 30001
        - name: learn-caddy-caddy
          image: learn-caddy:caddy
          ports:
            - containerPort: 80
  selector:
    matchLabels:
      app: learn-caddy

---
apiVersion: v1
kind: Service
metadata:
  name: learn-caddy
spec:
  ports:
    - port: 80
      nodePort: 80
      protocol: TCP
  selector:
    app: learn-caddy
  type: NodePort
```

在上面的yaml文件中，我定义了一个叫做`learn-caddy`的 Deployment，它里面有3个Container，这意味着它的每个Pod里面会**同时包含这三个容器**。这3个容器，分别是一个caddy容器（用作反向代理），和两个gin实现的简易web逻辑。

以及定义了一个Service，它将上面这个Deployment转发到 NodePort=80 上面供外部访问。也就是说，外部访问到的是caddy提供的服务。

那么 Sidecar 是如何体现的呢？我们看caddyfile：

```text
:80 {
    reverse_proxy /business* {
        to localhost:30001
    }
    reverse_proxy /auth* {
        to localhost:30000
    }
}
```

看仔细了~它反向代理的是`localhost`，而不是其他的服务的名字或者IP地址。这就是Pod共享网络资源的最好例子。

我们说用`Envoy`来做Sidecar，就是利用这个机制，Envoy服务和业务服务运行在同一个环境下，它们之间的通讯不需要经过TCP，而是直接内存复制，使得Envoy这一层额外的代理的开销降低到忽略不计的程度。

## 小结

呼~ 过年前总算是把k8s（的入门知识）摸索得差不多了，这个年可以安安心心地过了 ：）

总的来说呢，k8s在安装过程稍微有一些坑。虽然踩过一遍之后回头看，觉得这些坑都没什么难的；可是在没有踩过坑的情况下抹黑前进，的确是非常考验意志的一件事情（反正那天我是有点暴躁的）。

但是安装完成之后，在使用阶段我目前觉得还是挺顺手的。而且功能也的确是非常的强大。

满足~
