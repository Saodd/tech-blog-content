```yaml lw-blog-meta
title: OS学习笔记42：持久化：日志文件系统
date: "2019-09-04"
brev: 硬盘在发生故障时，如何保持数据的一致性？借鉴数据库系统的思想，先写日志，是目前的解决方案。
tags: [OS]
```


# 第四二章 <Crash Consistency: FSCK and Journaling>

[PDF链接](http://pages.cs.wisc.edu/~remzi/OSTEP/file-journaling.pdf)

前面章节介绍了，文件系统要管理一系列的数据结构来提供文件的抽象。并且，与内存中的数据不同的是，文件系统需要将文件持久地储存在设备中。

一个最主要的挑战就是，如何应对写入状态时突然遭遇的电源中断或者系统崩溃等问题。我们称为`故障一致性问题crash-consistency problem`。

**关键问题：如何避免故障的影响？假设故障可能发生在任意时间点，如何确保硬盘上的状态是合理的？**

## 42.1 详细的例子

首先假设工作负荷。假设一个最简单的情景：向一个已经存在的文件后面追加一个数据块。首先open，然后lseek到文件末端，然后申请写入4KB的块。

再假设我们的硬盘使用简单文件系统：

![Figure 42.1.1](https://saodd.github.io/tech-blog-pic/2019/2019-09-04-Fig-42-1-1.png)

现在索引节点I[v1]中的信息：

```text
owner       : remzi
permissions : read-write
size        : 1
pointer     : 4
pointer     : null
pointer     : null
pointer     : null
```

我们要向文件后面追加，需要更新硬盘的三个部分：索引节点，数据区块，数据区域的位图。更新后应该是这个样子：

![Figure 42.1.2](https://saodd.github.io/tech-blog-pic/2019/2019-09-04-Fig-42-1-2.png)

还要注意一点的是，用户的写入请求并不一定是立即执行的，有可能在内存中缓存一会儿。

### 故障场景

三个写入请求只执行了一个：

- 只写了数据区块。此时由于节点和位图都没变，看起来就像根本没写过一样。
- 只更新索引节点。此时索引节点中指向了一个垃圾数据区域，其中的数据是无效的；而且此时发生了`不一致问题inconsistency`，即索引节点与位图中的信息不匹配。
- 只更新了位图。除了不一致问题，还会导致`空间泄露space leak`。

三个写入请求执行了两个：

- 没有写数据区块。不一致问题没有了，但是指向了一个垃圾区域。
- 没更新位图。
- 没更新索引节点。

归根结底，我们最希望的是文件系统能够提供类似原子性的写入操作。但是，并不能。

## 42.2 方案1：文件系统检查器 File System Checker

早期的文件系统很简单，让故障发生，然后稍后再来修复它们。一个典型就是`fsck`。但是它不能解决所有问题，比如在文件系统一致但是指向了垃圾区块的情况，就无法修复。

它主要做这些事情：

- 检查超级区块。
- 检查空闲区块。扫描索引节点、间接指针区块，来描绘一个正确的位图，然后与硬盘中的位图进行比较。如果不一致，它信任节点中的信息。
- 检查节点信息。如果发现节点信息明显错误，就会清除掉整个节点。
- 检查节点引用计数。
- 检查重复指针。
- 检查坏区块。比如明显指向错误地址的指针。
- 检查目录信息。目录中的数据都是由文件系统自己生成的，因此有一定的方法可以检测。

可以看到这样一个工具是非常复杂的。而且有个根本性的问题：太慢了！想象你有个几TB的硬盘，甚至RAID……

## 42.3 方案2：预写式日志

目前最流行的解决方案，是借鉴自数据库系统的思路：`预写式日志Write-Ahead Logging`，在文件系统中我们称其为`日志journaling`，包括现在的ext3和NTFS都用这个。

基本思想很简单：在真正写入数据之前，先把你要做的事情记下来，记在硬盘上某个约定俗成的地方。在写入之前先写日志，因此得名。

通过这种方式，当故障发生了，你只需要看看日志就能知道你应该如何处理这次故障。当然，这会给写入性能造成一点点影响。

我们介绍一下ext3式如何使用日志的。比起之前介绍的简单文件系统，最大的区别就是多了一个`日志journal`区域。（这个区域有时会独立设置在另一个设备上，或者作为文件系统内的一个文件。）

![Figure 42.1.3](https://saodd.github.io/tech-blog-pic/2019/2019-09-04-Fig-42-1-3.png)

### 数据日志

还是用前面描述的那个经典的写入操作。在真正写入之前，先写日志：

![Figure 42.1.4](https://saodd.github.io/tech-blog-pic/2019/2019-09-04-Fig-42-1-4.png)

上面一共写了5个区块。第一个是`事务开始TxB`，其中包括一些关于这次操作的信息，以及`事务编号transaction identifier (TID)`。中间三块就是我们将要写入的数据本身，这种方式我们称为`物理日志`（相反的概念是`逻辑日志`，只保存一些关键信息，以此节约空间）。最后一块`事务结束TxE`用以编辑结束，也会包含事务编号。

当日志成功写入后，就开始正式的写入操作。这个过程我们称为`关卡checkpoint`。

因此整个流程就是：写日志，过关卡。

入果在写日志的时候故障了咋办？理想来说，我们希望同时请求写入五个日志区块；这样性能很快，但是硬盘内部可能会进行调度，实际写入顺序就未知了。

因此文件系统分两步走。先写入前面四个块，即除了TxE的前面四块。四块写入完成后，再写入TxE。这样就算中途崩溃了，那么在恢复过程中没有看见TxE区块，就说明这个日志是无效的。（注意，硬盘提供512bytes的原子性操作，因此TxE区块也应该设置为512bytes）

到目前为止，一个完整的写入请求包括三个步骤：日志写入、日志提交、过关卡。

### 恢复

现在问题简单了：如果日志没有提交，说明日志无效，那就只能跳过；如果日志提交了，那我们就可以根据日志来恢复数据。

### 日志打包

比如我们在同一个目录下同时新建两个文件，那么对于目录节点的写入操作就重复了，甚至有可能造成死循环。

因此现代的文件系统会有缓冲区，并把一批写入操作放在同一个全局事务中。

### 限定日志区域大小

在日志区域内进行`循环日志circular log`。实现方法就是，当数据已经确认写入（通过关卡）之后，释放掉日志区域的空间。
我们在日志区域前面留一个`超级日志区块journal superblock`，用以记录日志区的起点和终点信息。

到目前为止，一个完整的写入请求包括三个步骤：日志写入、日志提交、过关卡、释放日志。

### 元数据日志

目前所描述的日志文件系统还是很低效，因为每次写入都需要额外的操作并且有双倍的写入量。

因此有一些别的思路，比如`命令日志ordered journaling`（或者称`元数据日志metadata journaling`），它不记录用户数据区块，因此减少了很多日志量：

![Figure 42.1.5](https://saodd.github.io/tech-blog-pic/2019/2019-09-04-Fig-42-1-5.png)

在这种方式中，数据写入的顺序很重要。因为数据区块没有写在日志中，如果还像之前那样，写完日志事务之后再写数据区块，那就可能会面临问题：如果这个时候故障了，日志保证了文件系统的一致性，但是指向了一个垃圾区块。

因此将流程改变为：写入数据区块、写入元数据日志、提交日志、过关卡（更新元数据）、释放日志。

注意，整个过程中数据区块只写了一次，而元数据写了两次。

通过要求先写数据区块，文件系统能够保证指针不会指向垃圾区块。事实上，这种『先写目标对象，再写指针』的思路正是故障一致性问题的核心思想，这种思想在其他领域也得到了大量的应用。

这种元数据日志方案是目前正流行的，包括NTFS都在使用；而ext3可以让用户选择使用哪种方案。

### 边缘情况：区块复用

有人说：文件系统的阴暗面是什么？就是删除文件……所有与删除文件相关的操作都非常复杂。当你要删除区块并且重新分配给其他文件时，梦魇来了。

举个例子，我们有一个目录foo，用户在其中增加了一个文件，因此日志中要更新这个目录的节点信息以及数据信息，假设数据信息存放在1000号区块。然后用户将整个目录都删除了。然后又创建了一个文件bar，刚好复用了1000号区块，但是日志中只有节点的更新，数据是不会记录在日志中的。

![Figure 42.1.6](https://saodd.github.io/tech-blog-pic/2019/2019-09-04-Fig-42-1-6.png)

如果此时发生故障，在`重演replay`时，文件系统会将目录foo的数据写到1000号区块中，而这一块应该文件bar的数据区块！

解决办法有一些。比如，确保区块删除操作已经通过关卡之后，再复用这个区块。或者ext3的做法是，引入一个新的日志类型`revoke`，当重演时发现了标记为revoke的区块，就不对其进行相应的重演。

### 用图片小结一下流程

![Figure 42.1](https://saodd.github.io/tech-blog-pic/2019/2019-09-04-Fig-42-1.png)

![Figure 42.2](https://saodd.github.io/tech-blog-pic/2019/2019-09-04-Fig-42-2.png)

## 42.4 方案3：其他方案

前面介绍了两种解决发难，一种是懒惰方法，基于fsck；以及另一种更加积极的方法，基于日志。

有一种方法叫`软更新Soft Updates`，这种方法大概是小心地把所有写入任务进行排序，以保证一致性以及非指向垃圾区块。实现起来很难，首先要充分了解各种硬件。

有一种方法叫`写时复制copy-on-write (COW)`，在目前也有一些应用，比如ZFS文件系统。主要思想是不向原有地址中写入，而是写入一个新的空闲地址，确定完成后再更新目录树来指向新的地址。

一种方法是本书作者的团队开发的，称为`backpointer-based consistency (BBC)`。这种方法不要求写入顺序；因此为了实现一致性，给每个区块都设置一个额外的后台指针。当访问文件时，检查实际指针与后台指针是否一致，如果不一致就说明有问题。这是另一种懒惰的方法。

还有`optimistic crash consistency`，目标在于减少日志模型在写入时的等待次数。

## 42.5 小结

当硬盘从故障中恢复时，以前的方法是扫描整个硬盘，现在只需要扫描日志区。日志有两大类，一种是完全日志，一种是元数据日志（不包括数据区块）。日志文件系统以额外的写入延迟与空间损耗为代价，高效地实现了硬盘的一致性。

> 译者注：我的理解是，数据持久化的关键点在于『一致性』，在保证一致性的前提下，有时丢失一点点最后数据也是被容忍的。  
> 当然，这种容忍只是在硬盘设备的层面。如果应用程序（比如数据库）要求完全的可靠性，可以使用立即写入的系统调用，来真正确认数据是否已经被持久化了。
