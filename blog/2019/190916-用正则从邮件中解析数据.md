```json lw-blog-meta
{"title":"用正则从邮件中解析数据","date":"2019-09-16","brev":"使用正则与协程，高效地解析邮件。","tags":["Python"],"path":"blog/2019/190916-用正则从邮件中解析数据.md"}
```



## 业务背景

在前几天的博客中介绍了如何下载邮件并抽取出其中的文本。但是邮件文本并不是我们真正的目标数据，我们还要将文本转化为结构化的数据。

我们公司是一家私募基金公司，有着大量的OTC-FX交易。由于是OTC，所以交易结果由邮件确认，而各家交易对手的邮件格式大不相同（甚至同一家公司也会时不时地改变）。因此如何从邮件文本中提取数据是一个相当有挑战性的问题。

我们典型的邮件内容如下（隐去了真实业务信息，所有数字均为杜撰）：

```text
--------------
Trade Summary:
--------------

    Counter Party          :  xxxxxx
    Trade Date             :  2019-09-xx
    Value Date             :  2019-09-xx
    Trade Type             :  SPOT
    Deal Type              :  Client SELL
    Quote Basis            :  CCY2PERCCY1

    Currency1              :  USD
    Amount                 :  1,000,000.00
    
    Currency2              :  CNH
    Counter Amount         :  7,050,000.00

    Rate                   :  7.05000
    Mid Rate               :  7.05000
```

或者这样：

```text
AP xxx sells USD 1,000,000.00 & buys CNH 7,080,000.00 at 7.08000, value xx-Sep-2019
AP xxx sells USD 1,000,000.00 & buys CNH 7,080,000.00 at 7.08000, value xx-Sep-2019
```

目前见过的有十几种格式。

**关键问题：如何设计正则模板，使得匹配尽可能稳定？如何设计控制逻辑，使得模板选择尽可能灵活？**

## 最初设计

最直接能想到的办法，就是给每种邮件单独写一个模板。

我刚接手这项工作的时候，前任留给我的甚至只有一个Excel文件，里面用Excel函数实现了一些简单的识别逻辑。但是我很快就放弃了这个工具，因为识别成功率先不论、Excel函数实在太难维护了（试想一个嵌套了十几层的`IF()`函数就为了识别一个字段，没有代码格式化，没有IDE提示，谁能维护这种鬼东西？？）。

后来按照类似的思路，实现了一个python版本的。分为两大块，逻辑模块负责循环下载邮件，然后根据邮件的特征（标题、发件人等），从解析模块中选择一个模板函数来进行解析。8种模板，一共写了有一千行，全部我一个一个字敲上去的。

![千行代码](https://raw.githubusercontent.com/Saodd/Saodd.github.io.backup-Jun2020/master/static/blog/2019-09-16-thousand-lines.png)

## 问题与反思

从上面的图片中可以看到我的程序结构。我将整个邮件的内容传入某个解析函数，如果识别失败就会返回一个空的`pandas.Dataframe`对象，这样逻辑模块就会判断为解析失败，然后做出一些提示来让我人工解析。

```python
# 逻辑模块，根据特征选择解析函数：
if re.findall("generated by the BNP Paribas Cortex system", mail_content):
    df_new = self.parse_bnp_cortex(mail_content, df_new)
elif "Morgan FX & Commodities e-Trading" in mail_content:
    df_new = self.parse_jp_morgan(mail_content, df_new)
elif ("GSFX Recap" in mail_subject) and ("Goldman Sachs FX eDealing Disclaimer" in mail_content):
    df_new = self.parse_gs_fx_recap(mail_content, df_new)
elif ("GSFX - FILLED" in mail_subject) and ("Goldman Sachs FX eDealing Disclaimer" in mail_content):
    df_new = self.parse_gs_fx_filled(mail_content, df_new)
elif ("@ubs.com" in mail_from) and ("UBS" in mail_subject):
    df_new = self.parse_ubs_himson_tam(mail_body, df_new)
elif ("Karthik.Kamat@gs.com" in mail_from) and ("FX" in mail_subject):
    df_new = self.parse_gs_Kamat_Karthik(mail_body, df_new)
elif ("fxtrademail@ubs.com" in mail_from):
    df_new = self.parse_ubs_fxtrademail(mail_body, df_new)
elif re.search('Classification.*?Confidential', mail_content, flags=re.DOTALL):
    print("[Warning] Recognized as BNP CONFO mail, skipping this mail.")
elif "apcapitalinvestment.com" in mail_from:
    print("[Warning] Recognized as AP people's mail, skipping this mail.")
else:
    df_new, self.content_txt = self.cannot_parse(mail_body, df_new, self.content_txt)
```

在解析函数内部，设置了一个正则表达式来抽取信息，然后转化为格式化的数据：

```python
# 大概看一眼就好了，逻辑非常复杂
def parse_ubs_fxtrademail(self, mail_body, df_new):
    """Added on 2019-08-06"""
    mail_date = mail_body[2]
    mail_content = mail_body[3]
    logger.info("Parsing as template: ***  UBS fxtrademail *** .")
    df_new.loc[1, 'mark'] = ""
    # 整体匹配
    mat = re.findall("(Counter Party.+?)Please contact", mail_content, flags=re.DOTALL)
    if not mat:
        df_new.loc[1, 'mark'] = "something wrong"
        logger.error("cannot find mail_content mat.")
        return df_new
    else:
        mail_content = mat[0]
    print(mail_content)
    # 交易匹配
    re_pat = (
        "Trade Date(?P<trade_date>.+?)\n.*?"
        "Value Date(?P<settlement_date>.+?)\n.*?"
        "Client(?P<side>.+?)\n.*"
        "Currency1(?P<currency_1>.+?)Amount(?P<amount_1>.+?)Currency2(?P<currency_2>.+?)Counter Amount(?P<amount_2>.+?)"
        "Rate(?P<fx_rate>.+?)\n"
    )
    if not re.search(re_pat, mail_content, flags=re.DOTALL):
        df_new.loc[1, 'mark'] = "something wrong"
        logger.error("cannot find mail_content mat.")
        return df_new
    line = 1
    for mat in re.finditer(re_pat, mail_content, flags=re.DOTALL):
        dic = mat.groupdict()
        dic = {k: v.replace(":", "").replace("\n", "").strip() for k, v in dic.items()}
        logger.info(dic)
        # 读取A
        if "USD" in dic['currency_2']:
            df_new.loc[line, 'currency'] = "USD" + dic['currency_1']
        elif "USD" in dic['currency_1']:
            df_new.loc[line, 'currency'] = "USD" + dic['currency_2']
        else:
            logger.error("cannot found [currency].")
        # 读取B
        try:
            df_new.loc[line, 'settlement_date'] = datetime.strptime(dic['settlement_date'], "%Y-%m-%d").strftime(
                "%Y%m%d")
        except:
            logger.error("cannot found [settlement_date].")
        # 读取C
        df_new.loc[line, 'account'] = "ABHK"
        # 读取E
        df_new.loc[line, 'trader_id'] = "VC000"
        # 读取F
        if ("BUY" in dic['side']):
            if "USD" in dic['currency_1']:
                df_new.loc[line, 'side'] = "Buy"
            elif "USD" in dic['currency_2']:
                df_new.loc[line, 'side'] = "Sell"
        else:
            if "USD" in dic['currency_1']:
                df_new.loc[line, 'side'] = "Sell"
            elif "USD" in dic['currency_2']:
                df_new.loc[line, 'side'] = "Buy"
        # 读取G
        if "USD" in dic['currency_1']:
            df_new.loc[line, 'base_notional'] = "%.2f" % float(dic["amount_1"].replace(",",""))
        else:
            df_new.loc[line, 'base_notional'] = "%.2f" % float(dic["amount_2"].replace(",",""))
        # 读取H
        try:
            df_new.loc[line, 'trade_date'] = datetime.strptime(dic['trade_date'], "%Y-%m-%d").strftime("%Y%m%d")
        except:
            logger.error("cannot found [trade_date].")
        # 读取I
        df_new.loc[line, 'fx_rate'] = dic['fx_rate'].strip()
        # 读取J
        df_new.loc[line, 'counterparty'] = "UBS"
        # 读取K
        if df_new.loc[line, 'currency'] in ["xxxx", "xxxx"]:
            df_new.loc[line, 'deliverable'] = 'n'
        else:
            df_new.loc[line, 'deliverable'] = 'y'

        line += 1

    return df_new
```

那么这样的实现有什么问题呢？

首先，如果逻辑模块选择了错误的解析函数，那就一定解析失败了。

第二也是更重要的是，解析函数难以维护。可以看到，正则表达式模板与匹配之后的转化都在解析函数中，意味着每增加一个模板，都要配套相应的转化逻辑。

第三，性能较差。由于每次从邮箱中下载，然后抽取文本，然后根据条件筛选，然后根据正则表达式去匹配，整个过程是以肉眼可见的速度在运行着，十分不爽。

那么如何解决上面这些问题呢？

我的答案是：利用正则表达式的分组功能，将所有数据都匹配成相同的格式，然后由一个统一的函数来转化为我们所需的结构数据。同时，不要根据条件选择解析函数，而是让所有解析函数都走一遍，哪个成功了就用哪个。

那引入了一个新的问题：如果让所有的解析函数都走一遍，性能如何保证？

我的答案是：正则表达式有一个`编译`的过程，编译较慢，而编译之后的匹配是非常迅速的。我们利用协程（或者闭包）来保持这个编译后的对象。

## 新版实现

首先，我们将邮件下载下来，去除重复后保存在本地的Mongo数据库中（前面的博客介绍过如何实现）。

那么我们这个解析程序就首先要从Mongo中取出数据来：

```python
def main():
    # 解析器对象，由于我们要保持正则编译对象，所以用一个对象来持有
    parsor = Parsor()
    datas = []
    with MongoClient(host="192.168.1.242") as mg:
        mgcl = mg.get_database("Email").get_collection("MO_raw")
        docs = mgcl.find({"date": {"$gt": datetime.now() - timedelta(2)}},
                         {"_id": 1, "subject": 1, "body": 1, "date": 1})
        for doc in docs:
            # 将二进制邮件转化为文本
            text = read_text(doc["body"])
            # 将文本用正则表达式匹配，得到一个groupdict
            data = parsor.handle(doc["subject"], text)
            if len(data) == 0:
                # 匹配不成功，打印邮件文本
                logger.warning(text)
            else:
                # 匹配成功，添加到数组中
                datas.extend(data)
    # 将dict转化为Dataframe并输出
    save_datas(datas)
```

然后我们重点看一下解析器对象如何写：

```python
@staticmethod
def parse_ubs_fxtrademail():  # -> List[dict]
    pat1 = (
        "Trade Summary:.*?"
        "Value Date\s*?:\s*?(?P<date>[0-9a-zA-Z\-]{9,11}).*?"
        "Deal Type\s*?:\s*?Client (?P<bs1>[a-zA-Z]{3,6}).*?"
        "Currency1\s*?:\s*?(?P<ccy1>[a-zA-Z]{3})\s*?"
        "Amount\s*?:\s*?(?P<amount1>[0-9,.]+?)\s*?"
        "Currency2\s*?:\s*?(?P<ccy2>[a-zA-Z]{3})\s*?"
        "Counter Amount\s*?:\s*?(?P<amount2>[0-9,.]+?)\s*?"
        "Rate\s*?:\s*?(?P<rate>[0-9,.]+?)\s"
    )
    cp1 = re.compile(pat1, flags=re.DOTALL)

    data = []
    while True:
        subject, text = (yield data)
        data = [x.groupdict() for x in cp1.finditer(text)]
```

解析函数非常简单：一个正则表达式，一个生成器的循环。
注意，我在正则表达式中增加了大量的限制条件，为的是尽可能地精确匹配（因为匹配失败了还可以让下一个解析函数再试试）。

这样设置的话，我就可以非常容易地维护正则表达式了。

然后我们做一个操作入口：

```python
class Parsor(object):
    def __init__(self):
        parsors = [x for x in self.__dir__() if x.startswith("parse")]
        parsors = [getattr(Parsor, x)() for x in parsors]
        [p.__next__() for p in parsors]
        self.parsors = parsors

    def handle(self, subject: str, text: str) -> List[dict]:
        for p in self.parsors:
            data = p.send((subject, text))
            if len(data):
                return data
        return []
```

这样就大功告成了！
