```yaml lw-blog-meta
title: Celery练习：在多主机环境下的消息传递
date: "2019-06-28"
brev: 用docker来模拟一个分布式系统的环境来学习Celery框架。
tags: [中间件]
```


## Celery简介

先看看[官网](http://www.celeryproject.org/)

> Celery is an asynchronous task queue/job queue based on distributed message passing.	It is focused on real-time operation, but supports scheduling as well.  
> The execution units, called tasks, are executed concurrently on a single or more worker servers using multiprocessing, Eventlet,	or gevent. Tasks can execute asynchronously (in the background) or synchronously (wait until ready).  
> 大芹菜 是一个异步的任务队列，基于分布式的消息传递。它专注于实时任务处理，但是也支持定时任务。  
> 任务们会被一个或多个执行者执行，可以选择同步或者异步。

然后引用网上常见的图：

![Celery结构图](https://raw.githubusercontent.com/Saodd/Saodd.github.io.backup-Jun2020/master/static/blog/2019-06-28-CeleryStruct.png)

是典型的“生产-中介-消费”结构。废话不多少，开始实现！

## 准备环境

首先准备一个项目文件夹，我这里是`C:/Users/lewin/mycode/learnCelery`.

既然是在分布式系统中实现，至少也要两台电脑吧。
我这里直接用Docker容器实现了。我们建立两个容器，挂载我们准备好的项目文件夹，并加入含有Redis数据库的网络：

```shell-session
PS > docker run -v C:/Users/lewin/mycode/learnCelery:/s/ --net=apmos_default --name python1  -dit appython:1.01
3b78d52ab82cf89b58054318d5bf468858f4d21bb3bab3f2cf8eea602e7b24b1
PS > docker run -v C:/Users/lewin/mycode/learnCelery:/s/ --net=apmos_default --name python2  -dit appython:1.01
049fb185fe1eec39de778d17557bb56bc8dd157757526712d70621e5cbe29a3b
```

在容器中安装Celery环境(我这个容器的镜像中已经包含了很多第三方库了，如果你还缺了其他的就另外去补吧)：

```shell-session
PS > docker exec -it python1 bash
root@3b78d52ab82c:/scripts# ping apmos_redis_1
(...ping OK，结果省略)
root@3b78d52ab82c:/scripts# python -m pip install celery[redis]
(...安装完成)
```

此时我们的“分布式系统”长这样：

```shell-session
PS > docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                      NAMES
049fb185fe1e        appython:1.01       "/bin/sh -c bash"        9 minutes ago       Up 9 minutes                                   python2
3b78d52ab82c        appython:1.01       "/bin/sh -c bash"        10 minutes ago      Up 10 minutes                                  python1
4d7f0ce89902        redis               "docker-entrypoint.s…"   6 weeks ago         Up 13 days          0.0.0.0:20011->6379/tcp    apmos_redis_1
```

## 异步项目

### 写代码

```text
/s
├── celery_app
│   ├── __init__.py
│   ├── config.py
│   ├── task1.py
│   └── task2.py
└── client.py
```

`__init__.py`代码：

```python
from celery import Celery

app = Celery('demo')  # 创建 Celery 实例
app.config_from_object('celery_app.config')  # 通过 Celery 实例加载配置模块
```

`config.py`代码：

```python
BROKER_URL = 'redis://apmos_redis_1:6379'  # 指定 Broker
CELERY_RESULT_BACKEND = BROKER_URL + '/0'  # 指定 Backend

CELERY_TIMEZONE = 'Asia/Shanghai'  # 指定时区，默认是 UTC

CELERY_IMPORTS = (  # 指定导入的任务模块
    'celery_app.task1',
    'celery_app.task2'
)
```

`task1.py`代码：

```python
import time
from celery_app import app


@app.task
def add(x, y):
    time.sleep(2)
    return x + y
```

`task2.py`代码：

```python
import time
from celery_app import app


@app.task
def multiply(x, y):
    time.sleep(2)
    return x * y
```

`client.py`代码：

```python
import os, sys
# ------------------------- Project Environment -------------------------
def _find_root(n):
    if n > 0: return os.path.dirname(_find_root(n - 1))
    return os.path.abspath(__file__)


_path_project = _find_root(1)
if _path_project not in sys.path: sys.path.insert(0, _path_project)

# ------------------------- Functions -------------------------
from celery_app.task1 import add
from celery_app.task2 import multiply

add.apply_async(args=[10,20])
print("Sent task!")

multiply.apply_async(args=(32,32))
print("Sent task!")
```

### 执行

在`python1`容器中启动worker：

```shell-session
root@3b78d52ab82c:/s# celery -A celery_app worker --loglevel=info
/usr/local/lib/python3.7/site-packages/celery/platforms.py:801: RuntimeWarning: You're running the worker with superuser privileges: this is
absolutely not recommended!

Please specify a different user using the --uid option.

User information: uid=0 euid=0 gid=0 egid=0

  uid=uid, euid=euid, gid=gid, egid=egid,

 -------------- celery@3b78d52ab82c v4.3.0 (rhubarb)
---- **** -----
--- * ***  * -- Linux-4.9.125-linuxkit-x86_64-with-debian-9.9 2019-06-28 07:20:40
-- * - **** ---
- ** ---------- [config]
- ** ---------- .> app:         demo:0x7fd72348e2e8
- ** ---------- .> transport:   redis://apmos_redis_1:6379//
- ** ---------- .> results:     redis://apmos_redis_1:6379/0
- *** --- * --- .> concurrency: 2 (prefork)
-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)
--- ***** -----
 -------------- [queues]
                .> celery           exchange=celery(direct) key=celery


[tasks]
  . celery_app.task1.add
  . celery_app.task2.multiply

[2019-06-28 07:20:40,730: INFO/MainProcess] Connected to redis://apmos_redis_1:6379//
[2019-06-28 07:20:40,737: INFO/MainProcess] mingle: searching for neighbors
[2019-06-28 07:20:41,756: INFO/MainProcess] mingle: all alone
[2019-06-28 07:20:41,784: INFO/MainProcess] celery@3b78d52ab82c ready.
```

在`python2`容器中启动`client.py`发送任务(仅仅只是发送任务，并不阻塞，可以看到程序马上结束了)：

```shell-session
root@049fb185fe1e:/s# python client.py
Sent task!
Sent task!
```

然后我们会在`python1`容器中看到输出的日志（中间隔了2秒）：

```shell-session
[2019-06-28 07:21:12,932: INFO/MainProcess] Received task: celery_app.task1.add[b685f691-eada-4401-921b-f912aa734031]
[2019-06-28 07:21:12,936: INFO/MainProcess] Received task: celery_app.task2.multiply[225b7de7-422b-4a08-a963-802e49da3907]
[2019-06-28 07:21:14,956: INFO/ForkPoolWorker-1] Task celery_app.task2.multiply[225b7de7-422b-4a08-a963-802e49da3907] succeeded in 2.0184475001879036s: 1024
[2019-06-28 07:21:14,958: INFO/ForkPoolWorker-2] Task celery_app.task1.add[b685f691-eada-4401-921b-f912aa734031] succeeded in 2.0252610999159515s: 30
```

这样看来，异步任务就执行成功了。

### 看看数据库

我们再去Redis数据库中确认一下结果：

```shell-session
PS > docker exec -it apmos_redis_1 bash
root@4d7f0ce89902:/data# redis-cli
```

启动Worker时的数据库情况：

```shell-session
127.0.0.1:6379> keys *
1) "_kombu.binding.celeryev"
2) "_kombu.binding.celery"
3) "_kombu.binding.celery.pidbox"
4) "unacked_mutex"

127.0.0.1:6379> SMEMBERS "_kombu.binding.celeryev"
1) "worker.#\x06\x16\x06\x16celeryev.ba40d412-18d2-4f5c-9746-f07c44d8c039"
127.0.0.1:6379> SMEMBERS "_kombu.binding.celery"
1) "celery\x06\x16\x06\x16celery"
127.0.0.1:6379> SMEMBERS "_kombu.binding.celery.pidbox"
1) "\x06\x16\x06\x16celery@3b78d52ab82c.celery.pidbox"
127.0.0.1:6379> get "unacked_mutex"
"9648fe93-df10-46b4-add2-e8f84ae1dec5"
```

调用`client.py`之后的数据库情况：

```shell-session
127.0.0.1:6379> keys *
1) "celery-task-meta-f08dd6e4-6f92-4c12-aa39-8494ea9fe127"
2) "_kombu.binding.celeryev"
3) "_kombu.binding.celery.pidbox"
4) "unacked_mutex"
5) "celery-task-meta-0eae6a6b-d8c4-4902-85ff-3561d24caf98"
6) "_kombu.binding.celery"

127.0.0.1:6379> get "celery-task-meta-f08dd6e4-6f92-4c12-aa39-8494ea9fe127"
"{\"status\": \"SUCCESS\", \"result\": 30, \"traceback\": null, \"children\": [], \"task_id\": \"f08dd6e4-6f92-4c12-aa39-8494ea9fe127\", \"date_done\": \"2019-06-28T07:35:58.069555\"}"
127.0.0.1:6379> get "unacked_mutex"
"8bab5a7c-0a2d-4a93-bc62-f07843248a4f"
127.0.0.1:6379> get "celery-task-meta-0eae6a6b-d8c4-4902-85ff-3561d24caf98"
"{\"status\": \"SUCCESS\", \"result\": 1024, \"traceback\": null, \"children\": [], \"task_id\": \"0eae6a6b-d8c4-4902-85ff-3561d24caf98\", \"date_done\": \"2019-06-28T07:35:58.072432\"}"
```

## 定时项目

我们在`config.py`文件中添加定时任务，并导入相应的模块：

```python
from datetime import timedelta
from celery.schedules import crontab

BROKER_URL = 'redis://apmos_redis_1:6379'  # 指定 Broker
CELERY_RESULT_BACKEND = BROKER_URL + '/0'  # 指定 Backend

CELERY_TIMEZONE = 'Asia/Shanghai'  # 指定时区，默认是 UTC

CELERY_IMPORTS = (  # 指定导入的任务模块
    'celery_app.task1',
    'celery_app.task2'
)

CELERYBEAT_SCHEDULE = {
    'add-every-30-seconds': {
        'task': 'celery_app.task1.add',
        'schedule': timedelta(seconds=30),  # 每 30 秒执行一次
        'args': (5, 8)  # 任务函数参数
    },
}
```

依然保持刚才`python1`容器的`worker`进程，依然在`python2`容器中启动这个定时器：

```shell-session
celery beat v4.3.0 (rhubarb) is starting.
__    -    ... __   -        _
LocalTime -> 2019-06-28 07:51:51
Configuration ->
    . broker -> redis://apmos_redis_1:6379//
    . loader -> celery.loaders.app.AppLoader
    . scheduler -> celery.beat.PersistentScheduler
    . db -> celerybeat-schedule
    . logfile -> [stderr]@%WARNING
    . maxinterval -> 5.00 minutes (300s)
```

此时我们可以在`python1`容器的`worker`进程看到输出运行日志了。

## 小结

至此，我们就实现了在一个模拟分布式环境的情况下，利用`Redis`数据库作为中间人，
在两台主机上分别产生/消费异步任务的架构。  

利用这套机制，我们可以很轻松的拓展运行容量。比如Web服务，比如大数据计算，都是在可以想象的范畴以内了。