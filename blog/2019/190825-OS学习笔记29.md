```yaml lw-blog-meta
title: OS学习笔记29：并发：带锁的数据结构
date: "2019-08-25"
brev: 计数器、列表、队列、哈希表，粗浅看一下各有什么特征。
tags: [OS]
```


# 第二九章 <带锁的数据结构 Lock-based Concurrent Data Structures>

[PDF链接](http://pages.cs.wisc.edu/~remzi/OSTEP/threads-locks-usage.pdf)

并发最关键点在于对共享内存的保护，共享内存里是什么呢，那当然是数据了。我们用锁来实现`线程安全thread safe`的数据结构。

**关键问题：如何给数据结构加锁？如何保证高性能？**

## 29.1 并发计数器

最简单的并发数据结构就是一个计数器了吧。

```c
// 单线程版
typedef struct __counter_t {
    int value;
} counter_t;

void init(counter_t *c) { c->value = 0; }
void increment(counter_t *c) { c->value++; }
void decrement(counter_t *c) { c->value--; }
int get(counter_t *c) { return c->value; }
```

```c
// 带锁版
typedef struct __counter_t {
    int value;
    pthread_mutex_t lock;
} counter_t;

void init(counter_t *c) {
    c->value = 0;
    Pthread_mutex_init(&c->lock, NULL);
}

void increment(counter_t *c) {
    Pthread_mutex_lock(&c->lock);
    c->value++;
    Pthread_mutex_unlock(&c->lock);
}

void decrement(counter_t *c) {
    Pthread_mutex_lock(&c->lock);
    c->value--;
    Pthread_mutex_unlock(&c->lock);
}

int get(counter_t *c) {
    Pthread_mutex_lock(&c->lock);
    int rc = c->value;
    Pthread_mutex_unlock(&c->lock);
    return rc;
}
```

关于性能问题，只要你的数据结构不是太慢（没有成为明显的瓶颈）的话，就别去做优化了，保持简单是最重要的。

我们在4核i5处理器上运行，每个线程累加一百万次，运行时间如下图。单线程情况下只需要0.03秒，而两个线程就需要大约5秒，太可怕了。

![Figure 29.5](/static/blog/2019-08-25-Fig-29-5.png)

### 译者的小实验

为什么C语言的锁性能这么差？因为线程切换上下文的问题，还是因为OS的futex排队的问题？我试着写了一段Go代码来测试运行时间：

```go
func main() {
    var ct *counter = &counter{}
    var sig chan int = make(chan int)
    go countMillion(ct, sig, 1)
    go countMillion(ct, sig, 2)

    {
        start := time.Now()
        sig <- 1
        sig <- 1
        <-sig
        <-sig
        totalTime := time.Since(start)
        fmt.Println(totalTime.Seconds(), ct.num)
    }
}

func countMillion(ct *counter, sig chan int, name int) {
    <-sig
    for i := 0; i < 1000000; i++ {
        ct.increment()
    }
    sig <- 1
}

type counter struct {
    num  int
    lock sync.Mutex
}

func (self *counter) increment() {
    self.lock.Lock()
    self.num++
    self.lock.Unlock()
}
```

```text
// 本地机器直接运行
go version go1.12.5 windows/amd64
0.0339944 2000000

// Go容器内运行
go version go1.12.6 linux/amd64
0.0606376 2000000
```

首先，我两个线程（Go程）运行一百万次，时间也只需要0.03-0.06秒的时间；

其次，Linux的性能居然比windows更慢，不可思议！是因为docker的性能损耗？还是因为Go编译器在不同系统上的优化不同？还是因为两个系统提供的锁的性能差距？还是因为两个Go程启动时间的不同？

验证是否是锁的问题：其他代码不变，去掉结构体中的锁：

```text
Windows:    0.002028 1018748
Linux:      0.0024115 1109482
```

这下非常接近了，看来的确是OS提供的锁不同，或者是因为Docker虚拟了一次带来了性能损耗。

再排除一下是否是Go程启动时间的问题：其他代码不变，在`go countMillion(...)`后面一个`time.Sleep()`，让Go程有充分的时间去启动：

```text
Windows:    0.0088302 1033680
Linux:      0.0099855 1064616
```

这个时间的差距应该是纯粹的计算性能差距了，还算可以接受。（虽然理论上还是觉得不对，因为IBM的论文有过研究，docker对于性能的损耗只有1%-3%这个数量级。）

那是不是window环境下模拟linux容器带来的额外损耗呢？我找一台Linux机器来试一下：

```text
go version go1.12.9 linux/amd64  （容器内）
0.003231042 1067256
```

emmmmm...果然快了很多很多……由于我们（我可以用的）Linux服务器上没有Go环境，所以只有容器内的，没有宿主机的运行结果做对比（不过，同样的代码，在windows本地和Linux容器内的运行时间接近，可以认为性能相近）。下次有条件了再做个实验吧！

至于为什么Go的代码比C快，我认为这应该是线程与Go程的区别了。我们继续学下去，看下后面有什么说法：

### 可伸缩的计算 Scalable Counting

这个锁的问题非常严重，会严重影响Linux环境下的多核多线程性能。因此提出了`近似计数器approximate counter`。

近似计数器通过多个本地物理计数器(每个CPU内核一个)和一个全局计数器来表示一个逻辑计数器。每个线程只在自己的本地计数器上递增，这样就避免了冲突；然后周期性地将本地计数器的值汇总到全局计数器中：

![Figure 29.3](/static/blog/2019-08-25-Fig-29-3.png)

那么多久汇总一次呢？我们会设定一个`临界值threshold`；临界值越小，全局计数器越精确，但是性能会下降。上图中的临界值就是5.

性能如何？回到前面的图29.5，下面那条线就是近似计数器的性能表现（临界值设为1024），可以看到，这种粗糙的锁几乎没有多少的性能损耗。

再看看临界值与性能之间的正比关系：

![Figure 29.6](/static/blog/2019-08-25-Fig-29-6.png)

近似计数器的主要代码如下：

```c
typedef struct __counter_t {
    int global;                      // global count
    pthread_mutex_t glock;           // global lock
    int local[NUMCPUS];              // per-CPU count
    pthread_mutex_t llock[NUMCPUS];  // ... and locks
    int threshold;                   // update frequency
} counter_t;

// init: record threshold, init locks, init values
// of all local counts and global count
void init(counter_t *c, int threshold) {
    c->threshold = threshold;
    c->global = 0;
    pthread_mutex_init(&c->glock, NULL);
    int i;
    for (i = 0; i < NUMCPUS; i++) {
        c->local[i] = 0;
        pthread_mutex_init(&c->llock[i], NULL);
    }
}

// update: usually, just grab local lock and update
// local amount; once local count has risen ’threshold’,
// grab global lock and transfer local values to it
void update(counter_t *c, int threadID, int amt) {
    int cpu = threadID % NUMCPUS;
    pthread_mutex_lock(&c->llock[cpu]);
    c->local[cpu] += amt;
    if (c->local[cpu] >= c->threshold) {
        // transfer to global (assumes amt>0)
        pthread_mutex_lock(&c->glock);
        c->global += c->local[cpu];
        pthread_mutex_unlock(&c->glock);
        c->local[cpu] = 0;
    }
    pthread_mutex_unlock(&c->llock[cpu]);
}

// get: just return global amount (approximate)
int get(counter_t *c) {
    pthread_mutex_lock(&c->glock);
    int val = c->global;
    pthread_mutex_unlock(&c->glock);
    return val;  // only approximate!
}
```

> 注意，并不是并发数越多就能越快。如果锁设计得不好，可能还不如单线程！

## 29.2 并发数组

我们依然先看一个最基础的并发链表实现，这里我们只讨论insert：

```c
// basic node structure
typedef struct __node_t {
    int key;
    struct __node_t *next;
} node_t;

// basic list structure (one used per list)
typedef struct __list_t {
    node_t *head;
    pthread_mutex_t lock;
} list_t;

void List_Init(list_t *L) {
    L->head = NULL;
    pthread_mutex_init(&L->lock, NULL);
}

void List_Insert(list_t *L, int key) {
    // synchronization not needed
    node_t *new = malloc(sizeof(node_t));
    if (new == NULL) {
        perror("malloc");
        return;
    }
    new->key = key;

    // just lock critical section
    pthread_mutex_lock(&L->lock);
    new->next = L->head;
    L->head = new;
    pthread_mutex_unlock(&L->lock);
}

int List_Lookup(list_t *L, int key) {
    int rv = -1;
    pthread_mutex_lock(&L->lock);
    node_t *curr = L->head;
    while (curr) {
        if (curr->key == key) {
            rv = 0;
            break;
        }
        curr = curr->next;
    }
    pthread_mutex_unlock(&L->lock);
    return rv;  // now both success and failure
}
```

有个值得注意的问题是，在持有锁的期间的操作，如果抛出了异常，我们也要能够正确地释放锁，否则会引起死锁问题。
在insert函数中，我们在malloc时并没有拿锁，而是分配好内存之后，添加到末尾的时候才拿锁。

### 优化

`传递锁hand-over-hand locking`或者叫`耦合锁lock coupling`，主要思想是，不再为整个链表设置一把锁，而是给每个节点分别上锁。遍历链表时，先取下一个节点的锁，然后放掉当前节点的。

理论上很棒，但是现实中表现可能还不如一把简单全局锁。因为逐个地拿锁放锁的代价太大了。

## 29.3 并发队列

我们这次不用全局大锁，因为队列有两端，所以我们分别给头尾各设置一把锁：

```c
typedef struct __node_t {
    int value;
    struct __node_t *next;
} node_t;

typedef struct __queue_t {
    node_t *head;
    node_t *tail;
    pthread_mutex_t head_lock, tail_lock;
}

 void Queue_Init(queue_t *q) {
    node_t *tmp = malloc(sizeof(node_t));
    tmp->next = NULL;
    q->head = q->tail = tmp;
    pthread_mutex_init(&q->head_lock, NULL);
    pthread_mutex_init(&q->tail_lock, NULL);
}

void Queue_Enqueue(queue_t *q, int value) {
    node_t *tmp = malloc(sizeof(node_t));
    assert(tmp != NULL);
    tmp->value = value;
    tmp->next = NULL;

    pthread_mutex_lock(&q->tail_lock);
    q->tail->next = tmp;
    q->tail = tmp;
    pthread_mutex_unlock(&q->tail_lock);
}

int Queue_Dequeue(queue_t *q, int *value) {
    pthread_mutex_lock(&q->head_lock);
    node_t *tmp = q->head;
    node_t *new_head = tmp->next;
    if (new_head == NULL) {
        pthread_mutex_unlock(&q->head_lock);
        return -1;  // queue was empty
    }
    *value = new_head->value;
    q->head = new_head;
    pthread_mutex_unlock(&q->head_lock);
    free(tmp);
    return 0;
}
```

要注意的是，中间有一个哑节点，用来分割头和尾。

但是只给队列加锁还不够，我们日常中用到的，还需要能在队列为空（或满）时让请求的线程阻塞。下一章我们看。

## 29.4 并发哈希表

这里只给出一个单线程版的哈希表，请读者自己加锁！

```c
#define BUCKETS (101)

typedef struct __hash_t {
    list_t lists[BUCKETS];
} hash_t;

void Hash_Init(hash_t *H) {
    int i;
    for (i = 0; i < BUCKETS; i++) List_Init(&H->lists[i]);
}

int Hash_Insert(hash_t *H, int key) {
    return List_Insert(&H->lists[key % BUCKETS], key);
}

int Hash_Lookup(hash_t *H, int key) {
    return List_Lookup(&H->lists[key % BUCKETS], key);
}
```

并发哈希表的性能可以很高很高，因为每个`哈希桶hash bucket`都可以单独设置锁。

### 完成作业

```go
type myNode struct {
    key   int
    value int
}

type myHash struct {
    buckets [100][]*myNode
    locks   [100]sync.Mutex
}

func newMyHash() *myHash {
    tmp := new(myHash)
    for i := 0; i < 100; i++ {
        tmp.buckets[i] = []*myNode{}
    }
    return tmp
}

func (self *myHash) HashInsert(k, v int) {
    self.locks[k%100].Lock()
    defer self.locks[k%100].Unlock()

    bk := self.buckets[k%100]
    for i := range bk {
        if bk[i].key == k {
            bk[i].value = v
            return
        }
    }

    node := &myNode{k, v}
    self.buckets[k%100] = append(self.buckets[k%100], node)
}

func (self *myHash) HashLookup(k int) (int, bool) {
    self.locks[k%100].Lock()
    defer self.locks[k%100].Unlock()

    bk := self.buckets[k%100]
    for i := range bk {
        if bk[i].key == k {
            return bk[i].value, true
        }
    }
    return 0, false
}
```

简单测试：

```go
func main() {
    hs := newMyHash()
    for i := 0; i < 100; i++ {
        v := i * i
        hs.HashInsert(i, v)
    }
    for i := 0; i < 100; i++ {
        k, _ := hs.HashLookup(i)
        fmt.Println(i, k)
    }
}
```

## 29.5 小结

- 拿锁/放锁的时候一定要小心，不要死锁，也要包裹住所有的关键代码；
- 高并发并不一定意味着高性能，要避免不成熟的优化；

除了哈希表，还有一些数据结构也具备高并发潜力，比如`二叉树B-trees`（学一下数据库可能会让你对二叉树感悟深刻）。

甚至，还有一些不用传统锁的`非阻塞型数据结构non-blocking data structures`，这是一个更加深邃的领域。
