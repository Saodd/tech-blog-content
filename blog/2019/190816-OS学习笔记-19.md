```json lw-blog-meta
{"title":"OS学习笔记19：内存虚拟化-翻译缓存TLB","date":"2019-08-16","brev":"分页有个问题，每次虚拟地址的翻译，都要两次内存访问，无疑存在着巨大的性能问题。所以我们用缓存来改善这个性能的问题。","tags":["OS"],"path":"blog/2019/190816-OS学习笔记-19.md"}
```



# 第十九章 <分页：更快地翻译 Faster Translations (TLBs)>

[PDF链接](http://pages.cs.wisc.edu/~remzi/OSTEP/vm-tlbs.pdf)

**关键问题：如何加速地址翻译？硬件层面、OS层面分别需要什么样的支持？**

OS总是需要硬件支持才能提升性能，这次我们引入`翻译后备缓存traslation-lookaside buffer`（TLB），它是内存管理单元MMU的一部分，也是主流的地址翻译缓存，所以一个更好的名字是`地址翻译缓存address-translation cache`。

当需要翻译虚拟地址的时候，会先检查TLB中是否有，如果有缓存的话，那就非常快了。这种性能上的量变直接产生了质变（使得内存虚拟化真正变得可行）。

## 19.1 TLB基础算法

```
VPN = (VirtualAddress & VPN_MASK) >> SHIFT
(Success, TlbEntry) = TLB_Lookup(VPN)
if (Success == True) // TLB Hit
    if (CanAccess(TlbEntry.ProtectBits) == True)
        Offset = VirtualAddress & OFFSET_MASK
        PhysAddr = (TlbEntry.PFN << SHIFT) | Offset
        Register = AccessMemory(PhysAddr)
    else
        RaiseException(PROTECTION_FAULT)
else // TLB Miss
    PTEAddr = PTBR + (VPN * sizeof(PTE))
    PTE = AccessMemory(PTEAddr)
    if (PTE.Valid == False)
        RaiseException(SEGMENTATION_FAULT)
    else if (CanAccess(PTE.ProtectBits) == False)
        RaiseException(PROTECTION_FAULT)
    else
        TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
        RetryInstruction()
```

成功了我们称为`命中TLB hit`；失败了称为`TBL miss`，会从内存中读取，并且插入缓存中。

缓存的速度非常快，可以认为缓存命中的情况下，性能损耗非常小。所以我们提升性能的目标就是减少未命中缓存的情况。

## 19.2 例子：访问数组

```c
int sum = 0;
for (i = 0; i < 10; i++) {
    sum += a[i];
}
```

假设我们需要一个10个int32元素的数组；假设我们内存每页只有16bytes。那么地址空间长这样：

![Figure 19.2](https://cdn.jsdelivr.net/gh/Saodd/tech-blog-pic@gh-pages/2019/2019-08-16-Fig-19-2.png)

暂时忽略变量`i`和`sum`。当CPU访问`a[0]`时，由于是第一次访问这一页，会有TLBmiss；于是这一页被存入缓存中。当访问`a[1]`时，就可以从TLB中读取了。

总结一下命中情况：miss, hit, hit, miss, hit, hit, hit, miss, hit, hit，所以我们有`命中率hit rate`的指标了，在这个例子中就是70%。当然，如果页面大小增加到32bytes，那么命中率又提升了。

一般情况下，典型的页面大小是4KB，所以一般来说TLB的命中率相当可观。

假如页面大小足够大，使得TLB完全命中，我们称这种情况为`时间局限性temporal locality`（除此之外还有`空间局限性spatial`），而这种局限性是由应用程序提供的。也就是说，如果我们需要极致性能，我们可以在时空局限性上做一些优化。

> 时间局限性：最近访问的数据很可能马上被再次访问，比如循环。
> 空间局限性：如果访问了地址x，那么x周围的数据都可能马上被访问。
> 为什么不把缓存做大？因为我们受到物理法则的限制（比如光速），做不大。

## 19.3 谁来处理miss

有两个答案：硬件或者OS。

在早期，那时还是`复杂指令集CISC`，完全由硬件支持。那么硬件需要知道页码表放在哪（通过`页码表基础寄存器pagetable base register`），如果miss了，硬件要`遍历walk`一次页码表来找到未缓存的页码，再更新缓存，再重试指令。

一个例子就是Intel x86架构，就是由一套硬件管理的TLB以及`固定的多级页码表fixed multilevel page table`

而现代的架构（使用`RISC`或者简化指令集）是使用软件管理的TLB。当miss的时候，引起一个异常，然后进入内核态，通过陷阱指令处理程序使用特权指令来进行操作。

注意，这个return-from-trap指令跟我们之前说的有点不同。之前我们说的systemcall的情况，会返回到OS的环境去运行；而TLB的返回陷阱指令，结束后是继续回到进程中执行的。所以硬件要能够区分陷阱指令的类型。

注意，要小心无限循环。比如把陷阱指令放在未被翻译的内存中；或者使用一些永久有效的翻译手段。

软件管理TLB的好处就是灵活，可以使用任意的数据结构；然后就是简单，不需要太复杂的硬件支持。

## 19.4 TLB的内容

典型的TLB可能会有32，64或者128个位置，并被称为是`完全联结fully associative`。意味着每条翻译可能存在于TLB的任何位置，硬件必须同时并行地搜索整个TLB。

一条翻译可能长这样：` VPN | PFN | other bits `.

有趣的是“other bits”。比如经常会有一个`验证位valid bits`，用来说明这条翻译是否有效（比如上下文切换时全部设置为0）.还有保护位，地址空间辨认位，脏位等等。

## 19.5 上下文切换时的问题

TLB只留有针对特定进程的页码翻译缓存（每个进程的页码表是不同的），所以要防止下一个进程使用上一个进程的翻译表。

一个办法是`刷新flush`所有的TLB内容。如果是软件管理，那会需要一条显式的指令；如果是硬件管理，就可以与上下文切换时的一些动作关联起来。但是这样性能损耗会比较大。

所以有了一些硬件支持来减少损耗。一种办法是`地址空间辨识器address space identifier(ASID)`，或者你可以认为是`进程辨识器process identifier(PID)`。

![Figure 19.2.2](https://cdn.jsdelivr.net/gh/Saodd/tech-blog-pic@gh-pages/2019/2019-08-16-Fig-19-2-2.png)

这样就不会搞错了，并且还支持不同地址空间对同一个物理页的共享。不过OS就必须在上下文切换的时候写入当前正在运行的进程的ASID。

## 19.6 替代政策

另一种问题就是`缓存替换cache replacement`。当我们需要缓存一条新的翻译的时候，意味着旧的要被替换了。换哪一个？

我们的目标当然是保持命中率最大。我们会在后面的章节学习，其中一个方法是`最近最少使用least-recently-used`（LRU），可以最大化利用时空局限性；另一种方法就是`随机random`，它简单并且能规避边缘特殊情况。

## 19.7 真正的TLB

介绍一下MIPS R4000系统的TLB内容，它是一种软件管理的TLB。它支持32位地址以及4KB页面，即20位VPN和12位offset，但是系统保留了1位页码，VPN只能用19位。翻译后为24位PFN，所以支持64GB的物理内存。

![Figure 19.4](https://cdn.jsdelivr.net/gh/Saodd/tech-blog-pic@gh-pages/2019/2019-08-16-Fig-19-4.png)

- 有1位很有意思，叫`全局位global bit`，用于进程间全局共享。
- 8位ASID，用于鉴别地址空间。
- 3位`连贯位Coherence bits`，说明这一页是如何缓存的。
- 1位脏位，说明它是否被写入。
- 1位有效位。
- 还有`页码掩码page mask`这里没显示出来，它支持不同长度的页码长度。还有一些剩下的位数没有使用。

## 19.8 小结

通过TLB进行缓存，损耗降低到了最小，在正常情况下基本无感。

但是比如程序在短时间内访问了大量的页，超过了TLB的容量，就会频繁miss，即所谓的`超过容量TLB coverage`。我们后面会介绍大量容量页，特别有利于`数据库database manafement ststem(DBMS)`。

另一个问题，就是TLB容易成为CPU管道的性能瓶颈，特别是对于`物理索引缓存`而言。然后人们又必须想更多的办法来优化。
