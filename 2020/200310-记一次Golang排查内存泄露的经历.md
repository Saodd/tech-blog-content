```yaml lw-blog-meta
title: 记一次Golang排查内存泄露的经历
date: "2020-03-10"
brev: 系统部署在Docker上线运行，没想到居然发现了“内存泄露”问题！虽然最后调查结论是虚惊一场，不过整个调查过程还是挺有意思的。
tags: [Golang]
```


## 内存泄露的问题表现

这次的应用是一个数据录制系统，主要功能是订阅外部的websocket连接，然后把接收到的数据保存到本地文件。

按理来说，功能并不复杂，写得时候也很用心（很少第三方库，基本都是自己撸的代码），而且都用了`context`和`WaitGroup`来控制资源释放。所以刚刚发现问题的时候让我觉得非常意外。

最初发现问题是在`docker stats`命令下。通过这个命令我们可以查看容器当前的运行状态，包括运行内存。运行我们系统的alpine容器，刚启动时30+MB，第二天60+，第三天90+……一周后能达到200+MB，虽然看起来泄露量并不大，但是考虑到系统还要进一步开发并添加更多的功能，所以目前这个泄漏量也必须要找到原因。

## 病因猜想

一开始怀疑是`time.Timer`泄露了。因为在系统中有很多计时的逻辑，有大量的`NewTimer()`和`t.Reset()`等操作。而`time`标准库的文档中提了一句在某种情况下可能会泄露，因此我担心是不是我的用法不规范导致了`Timer`的泄露。

然后怀疑是`http`包里的`response.Body`泄露了。后来去除了REST接口的请求功能，只保留WebSocket接口的，但是这个问题依然存在，因此pass。

再接着就怀疑是不是`os.File`泄露啦，是不是`bufio`的缓冲区泄露啦，是不是哪里有全局变量挂住了引用啦，甚至是不是这个版本的GC有bug啊，内存释放了但是没有归还系统啊，内存的统计口径啊balabala……

随着思考的深入，这些疑问也一个个地消除。

## 探索经过

### 1. 常规工具pprof

说到Golang的内存诊断工具，唯一的首选当然是内置的`pprof`包了。具体使用方法在我另一篇博客里有，不赘述。

通过`pprof`统计的heap大小一直保持在20MB左右。但是，heap数量却在稳步上升，像这样：

```text
| 256 | allocs       |   -->    | 488 | allocs       |
|-----|--------------|          |-----|--------------|
| 0   | block        |          | 0   | block        |
| 0   | cmdline      |          | 0   | cmdline      |
| 287 | goroutine    |          | 286 | goroutine    |
| 256 | heap         |   -->    | 488 | heap         |
| 0   | mutex        |          | 0   | mutex        |
| 0   | profile      |          | 0   | profile      |
| 24  | threadcreate |          | 25  | threadcreate |
| 0   | trace        |          | 0   | trace        |
```

这个数字的含义我并没有搞清楚。但是通过`go tool`画出的图可以直观的看出，heap的大小并没有变化。因此常规工具宣告失败。

我甚至在想，`pprof`难道并不靠谱？

我甚至还产生了一个奇怪的念头，难道在stack上泄露了内存？？

### 2. 探索GC

参考文章： [How we tracked down (what seemed like) a memory leak in one of our Go microservices](https://blog.detectify.com/2019/09/05/how-we-tracked-down-a-memory-leak-in-one-of-our-go-microservices/)

文中猜测是go的垃圾回收并不会立即将多余内存还给系统。可以尝试一下`GODEBUG=madvdontneed=1`参数和显式调用`FreeOSMemory()`。经过我的实验发现：

- 不设置`GODEBUG=madvdontneed=1`的情况下，显式调用`FreeOSMemory()`不能观察到效果。
- 设置`GODEBUG=madvdontneed=1`的情况下，显式调用`FreeOSMemory()`可以释放1-2MB内存。但是长期来看占用内存依然在稳定上升。

这样就说明应该不是“释放的内存没有及时归还系统”的问题了。

接下来看看是不是“在stack上泄露内存”的问题。

我用`http`服务做了个接口，访问这个接口可以把整个系统的业务逻辑停下来，代码返回并阻塞在`main()`上。这样理论上来说，所有的stack都应该释放了。

观察结果是，只剩下个别全局变量占用的go程，其他go程都退出了。但是从`docker stats`观察，容器占用的内存并没有减少，还是与业务模块停止之前一样。

```text
| 101 | allocs       |
|-----|--------------|
| 0   | block        |
| 0   | cmdline      |
| 7   | goroutine    |
| 101 | heap         |
| 0   | mutex        |
| 0   | profile      |
| 85  | threadcreate |
| 0   | trace        |
```

这下又排除了stack和全局变量的问题。

### 3. 统计口径

思考了很久，看了很多文章。我忽然在想，“占用内存”到底是个什么定义？

学过操作系统原理，知道进程内存都是虚拟化的。有映射，也有公共库函数的共享。那么，一个进程所占用的内存到底是如何统计的？

于是我开始怀疑是不是docker stats命令本身的统计口径有问题。
参考github-issue: [Docker stats memory usage is misleading](https://github.com/moby/moby/issues/10824)

但由于现在系统运行的基础容器是alpine，很多统计工具都是简化版的。于是我尝试使用ubuntu作为基准镜像，看看能不能获取更多信息。然后，我终于意识到了`VSZ`和`RSS`的区别：

```text
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1 10.0  0.2 113028 47944 pts/0    Ssl+ Mar05 114:39 /scd-cc/cc -m market
```

容器内观察到的RSS一直在上升，但是RSS似乎并不能代表真实内存的变化情况。另外，有趣的是，在容器外部通过`docker stats`观察，打印出的MEM与内部RSS并不相等，而且MEM的增加速度要快很多。

而VSZ的数值一直稳定不变。进一步了解得知，似乎VSZ才是内存泄露的参考指标。如果是这样的话，目前我们的系统就不存在肉眼可以观察到的内存泄露问题。

参考： [pmap学习：系统测试中怎么确定内存泄露](https://blog.csdn.net/xiaofei0859/article/details/77449309)。
额外阅读： [Go 应用内存占用太多，让排查？](https://book.eddycjy.com/golang/talk/why-vsz-large.html)

然后我让系统继续运行更长的时间。经过一周以上的观察，确认了如下现象：

- VSZ稳定在113MB的位置，有一点点可以忽略不计的增长。
- RSS一直增长，但是最后稳定在48MB左右。
- 外部MEM增长更快，但是最后稳定在210MB左右。

## 结论

那么最后我觉得可以得出结论了。目前这个系统并没有内存泄露问题。虚惊一场。
